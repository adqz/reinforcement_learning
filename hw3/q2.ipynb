{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import time\n",
    "import pybulletgym  # register PyBullet enviroments with open ai gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from statistics import mean\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "\n",
    "np.set_printoptions(suppress=True, precision=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes, lr):\n",
    "        super(Policy, self).__init__()\n",
    "        self.fc1 = nn.Linear(8, 64)\n",
    "        self.hidden = nn.Linear(64, 64)\n",
    "        self.fc2 = nn.Linear(64, num_classes)\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "        self.sigma0 = nn.Parameter(torch.tensor([0.1]))\n",
    "        self.sigma1 = nn.Parameter(torch.tensor([0.1]))\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = self.tanh(x)\n",
    "        x = self.hidden(x)\n",
    "        x = self.tanh(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.tanh(x)\n",
    "        \n",
    "        sigma0 = torch.relu(self.sigma0)\n",
    "        sigma1 = torch.relu(self.sigma1)\n",
    "        \n",
    "        if sigma0 <= 0.001: sigma0 = sigma0 + 0.001\n",
    "        if sigma1 <= 0.001: sigma1 = sigma1 + 0.001\n",
    "\n",
    "        return x, torch.diag(torch.tensor([sigma0, sigma1]))\n",
    "\n",
    "def makePolicyNN(num_actions=2, lr=0.01):\n",
    "    ''' Initialize the policy class '''\n",
    "    assert isinstance(num_actions, int) and num_actions>0\n",
    "    \n",
    "    return Policy(num_actions, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyGradient:\n",
    "    def __init__(self, gamma, N=500, max_steps=1000):\n",
    "        ''' Initialize the Reacher PyBullet environment '''\n",
    "        assert isinstance(gamma, float) and 0.0<gamma<1.0, 'Invalid gamma'\n",
    "        assert isinstance(N, int) and N>0\n",
    "        assert isinstance(max_steps, int) and max_steps>0\n",
    "        \n",
    "        self.env = gym.make(\"modified_gym_env:ReacherPyBulletEnv-v1\", rand_init=False)\n",
    "        self.gamma = gamma\n",
    "        self.N = N\n",
    "        self.max_steps = max_steps\n",
    "        self.numActions = 2\n",
    "        self.numObs = 8\n",
    "        self.num_steps = 0\n",
    "        self.max_steps_reached = False\n",
    "        \n",
    "    def saveModel(self, index):\n",
    "        ''' Saves the policy NN'''\n",
    "        filename = 'q2_policy' + str(index) + '.pth.tar'\n",
    "        torch.save(self.state, filename)\n",
    "    \n",
    "#     def getSigma(self, policy_network):\n",
    "#         ''' Return sigma matrix of type tensor '''\n",
    "#         #extract the value from the object of type parameter\n",
    "#         sigma0 = policy_network.sigma0.data\n",
    "#         sigma0 = torch.relu(sigma0)\n",
    "        \n",
    "#         sigma1 = policy_network.sigma1.data\n",
    "#         sigma1 = torch.relu(sigma1)\n",
    "        \n",
    "#         # making sure each sigma value is not less than 1e-3\n",
    "#         if sigma0 < 1e-3: sigma0 = 1e-3\n",
    "#         if sigma1 < 1e-3: sigma1 = 1e-3\n",
    "        \n",
    "#         return torch.diag([sigma0, sigma1])\n",
    "    \n",
    "    def getAction(self, policy_network, state):\n",
    "        ''' Return an action from a stochastic policy '''\n",
    "        assert isinstance(state, np.ndarray) and len(state) == self.numObs\n",
    "        \n",
    "        state = torch.from_numpy(state).float()\n",
    "        torque_mean, torque_sigma = policy_network(state) #forward pass\n",
    "        \n",
    "        # Sampling from the 2D Gaussian and calculating the actions log probability\n",
    "        m = MultivariateNormal(torque_mean, torque_sigma)\n",
    "        action = m.sample() #type tensor\n",
    "        log_prob_of_action = m.log_prob(action) #type tensor\n",
    "\n",
    "        return action, log_prob_of_action\n",
    "    \n",
    "    def runEpisode(self, policy_network):\n",
    "        ''' Generate [s_t, a_t, r_t] pairs for one episode '''\n",
    "        \n",
    "        state = self.env.reset()\n",
    "        done = False\n",
    "        rewards, log_prob_of_actions = [], []\n",
    "        while not(done):\n",
    "            a, lpa = self.getAction(policy_network, state)\n",
    "            state, r, done, info = self.env.step(np.array(a))\n",
    "            r = rewards.append(r)\n",
    "            log_prob_of_actions.append(lpa)\n",
    "            self.num_steps += 1 #incrementing total number of steps in one iteration\n",
    "            if self.num_steps >= self.max_steps: #checking\n",
    "                self.max_steps_reached = True\n",
    "                break;\n",
    "        \n",
    "        return rewards, log_prob_of_actions\n",
    "    \n",
    "    def doReinforcePart1(self, policy_network, verbose=False):\n",
    "        ''' Improve policy by implementing vanilla version of Reinforce algo '''\n",
    "        \n",
    "        gamma = self.gamma\n",
    "        arr_objective = []\n",
    "        avg_returns = np.zeros((self.N,))\n",
    "        num_episodes = np.zeros((self.N,))\n",
    "        sigma = np.zeros((self.N, 2))\n",
    "        \n",
    "        for i in range(self.N): #improving policy for \"self.N\" number of iterations\n",
    "            n, objective = 0, 0\n",
    "            \n",
    "            # Resetting variables\n",
    "            self.num_steps = 0\n",
    "            self.max_steps_reached = False\n",
    "            \n",
    "            # Looping until a maximum number of steps are taken\n",
    "            while not(self.max_steps_reached):\n",
    "                # Run an episode with policy and count number of steps taken\n",
    "                r, log_prob_of_actions = self.runEpisode(policy_network)\n",
    "                n += 1 #increasing the episode count by 1\n",
    "                \n",
    "                # Calculate discounted return (G_tau) and summation of log probs of actions from the episode\n",
    "                G_tau = sum([ r[t] * gamma**t for t in range(len(r))])\n",
    "                sum_lpa = sum(log_prob_of_actions)\n",
    "                avg_returns[i] += sum(r)\n",
    "                \n",
    "                # Calculate objective\n",
    "                objective += G_tau * sum_lpa\n",
    "                assert isinstance(objective, torch.Tensor)\n",
    "            \n",
    "            avg_returns[i] /= n\n",
    "            num_episodes[i] = n\n",
    "            sigma0, sigma1 = policy_network.sigma0.data.item(), policy_network.sigma1.data.item()\n",
    "            sigma[i,:] = np.array([sigma0, sigma1])\n",
    "            \n",
    "            # Updating policy\n",
    "            policy_network.optimizer.zero_grad()\n",
    "            objective = -objective/n #averaging over n episodes and flipping sign so it does gradient ascent\n",
    "            objective.backward()\n",
    "            policy_network.optimizer.step()\n",
    "            arr_objective.append(objective.item())\n",
    "\n",
    "            if verbose and (i%5 == 0):\n",
    "                print('Iteration: {0} \\t Objective: {1:.3f} \\t Average reward: {2:.3f} \\t Num_episodes: {3}'\\\n",
    "                      .format(i, objective, avg_returns[i], num_episodes[i]))\n",
    "                \n",
    "        self.state = { 'state_dict': policy_network.state_dict(),\n",
    "                      'optimizer': policy_network.optimizer.state_dict() }\n",
    "\n",
    "        return policy_network, arr_objective, avg_returns, num_episodes, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "options= \n",
      "Iteration: 0 \t Objective: -159.864 \t Average reward: -26.967 \t Num_episodes: 21.0\n",
      "Iteration: 5 \t Objective: -164.057 \t Average reward: -27.251 \t Num_episodes: 21.0\n",
      "Iteration: 10 \t Objective: -150.292 \t Average reward: -26.159 \t Num_episodes: 22.0\n",
      "Iteration: 15 \t Objective: -139.690 \t Average reward: -27.179 \t Num_episodes: 23.0\n",
      "Iteration: 20 \t Objective: -145.278 \t Average reward: -26.804 \t Num_episodes: 21.0\n",
      "Iteration: 25 \t Objective: -130.501 \t Average reward: -25.007 \t Num_episodes: 24.0\n",
      "Iteration: 30 \t Objective: -149.344 \t Average reward: -27.901 \t Num_episodes: 21.0\n",
      "Iteration: 35 \t Objective: -150.483 \t Average reward: -31.208 \t Num_episodes: 20.0\n",
      "Iteration: 40 \t Objective: -159.771 \t Average reward: -31.942 \t Num_episodes: 20.0\n",
      "Iteration: 45 \t Objective: -137.978 \t Average reward: -29.539 \t Num_episodes: 22.0\n",
      "Iteration: 50 \t Objective: -154.424 \t Average reward: -32.620 \t Num_episodes: 20.0\n",
      "Iteration: 55 \t Objective: -150.200 \t Average reward: -31.036 \t Num_episodes: 21.0\n",
      "Iteration: 60 \t Objective: -166.456 \t Average reward: -32.954 \t Num_episodes: 20.0\n",
      "Iteration: 65 \t Objective: -151.348 \t Average reward: -31.753 \t Num_episodes: 20.0\n",
      "Iteration: 70 \t Objective: -150.089 \t Average reward: -29.646 \t Num_episodes: 21.0\n",
      "Iteration: 75 \t Objective: -157.569 \t Average reward: -31.142 \t Num_episodes: 20.0\n",
      "Iteration: 80 \t Objective: -156.537 \t Average reward: -31.634 \t Num_episodes: 21.0\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "render_env = True\n",
    "plot = True\n",
    "\n",
    "policy_network = makePolicyNN(num_actions=2, lr=1e-3)\n",
    "pg = PolicyGradient(gamma=0.9, N = 100, max_steps=3000)\n",
    "policy_network, arr_objective, avg_returns, num_episodes, sigma = pg.doReinforcePart1(policy_network, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if plot:\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(10,10))\n",
    "    axs = axs.flatten()\n",
    "    axs[0].plot(arr_objective)\n",
    "    axs[0].set_xlabel('Iteration')\n",
    "    axs[0].set_ylabel('Objective')\n",
    "    axs[0].set_title(' Objective vs iterations ')\n",
    "    axs[1].plot(avg_returns)\n",
    "    axs[1].set_xlabel('Iteration')\n",
    "    axs[1].set_ylabel('Average Rewards')\n",
    "    axs[1].set_title(' Average Rewards vs iterations ')\n",
    "    axs[2].plot(num_episodes)\n",
    "    axs[2].set_title(' Number of episodes vs iterations ')\n",
    "    axs[3].plot(sigma[:,0], '-r')\n",
    "    axs[3].plot(sigma[:,1], '-b')\n",
    "    axs[3].set_title(' Sigma values vs iterations ')\n",
    "    axs[3].legend(['sigma1', 'sigma2'])\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if render_env:\n",
    "    env = gym.make(\"modified_gym_env:ReacherPyBulletEnv-v1\", rand_init=False)\n",
    "    steps = 0\n",
    "    env.render('human')\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    time.sleep(3)\n",
    "    while steps<300:\n",
    "        a, _ = pg.getAction(policy_network, state)\n",
    "        state, r, done, info = env.step(np.array(a))\n",
    "        steps+=1\n",
    "        env.render('human')\n",
    "        time.sleep(0.1)\n",
    "    env.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# policy_network.saveModel(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ece276c_venv",
   "language": "python",
   "name": "ece276c_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
