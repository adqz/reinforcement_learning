{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from statistics import mean\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions.categorical import Categorical\n",
    "\n",
    "np.set_printoptions(suppress=True, precision=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes, lr):\n",
    "        super(Policy, self).__init__()\n",
    "        self.fc1 = nn.Linear(4, 128)\n",
    "#         self.fc2 = nn.Linear(16, 16)\n",
    "        self.fc3 = nn.Linear(128, num_classes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.softmax = nn.Softmax(dim=0) #important param to set --> dim\n",
    "        self.optimizer = optim.SGD(self.parameters(), lr=lr)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.softmax(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "def makePolicyNN(num_actions=2, lr=0.01):\n",
    "    ''' Initialize the policy class '''\n",
    "    assert isinstance(num_actions, int) and num_actions>0\n",
    "    \n",
    "    return Policy(num_actions, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyGradient:\n",
    "    def __init__(self, gamma, N=200, max_steps=500):\n",
    "        ''' Initialize the cart-pole environment '''\n",
    "        assert isinstance(gamma, float) and 0.0<gamma<1.0, 'Invalid gamma'\n",
    "        assert isinstance(N, int) and N>0\n",
    "        assert isinstance(max_steps, int) and max_steps>0\n",
    "        \n",
    "        self.env = gym.make (\"CartPole-v1\")\n",
    "        self.gamma = gamma\n",
    "        self.N = N\n",
    "        self.max_steps = max_steps\n",
    "        self.numActions = self.env.action_space.n\n",
    "        self.num_steps = 0\n",
    "        self.max_steps_reached = False\n",
    "    \n",
    "    def getAction(self, policy_network, state):\n",
    "        ''' Return an action from a stochastic policy '''\n",
    "        assert isinstance(state, np.ndarray) and len(state) == 4\n",
    "        \n",
    "        state = torch.from_numpy(state).float()\n",
    "        probs = policy_network(state)\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample() #type tensor\n",
    "        log_prob_of_action = m.log_prob(action) #type tensor\n",
    "        action = action.item() #extracting value from tensor\n",
    "        \n",
    "        assert isinstance(action, int) and action in [0,1]\n",
    "        \n",
    "        return action, log_prob_of_action\n",
    "    \n",
    "    def runEpisode(self, policy_network):\n",
    "        ''' Generate [s_t, a_t, r_t] pairs for one episode '''\n",
    "        \n",
    "        state = self.env.reset()\n",
    "        done = False\n",
    "        rewards, log_prob_of_actions = [], []\n",
    "        while not(done):\n",
    "            a, lpa = self.getAction(policy_network, state)\n",
    "            state, r, done, info = self.env.step(a)\n",
    "            r = rewards.append(r)\n",
    "            log_prob_of_actions.append(lpa)\n",
    "            self.num_steps += 1 #incrementing total number of steps in one iteration\n",
    "            if self.num_steps >= self.max_steps: #checking\n",
    "                self.max_steps_reached = True\n",
    "                break;\n",
    "        \n",
    "        return rewards, log_prob_of_actions    \n",
    "    \n",
    "    def doReinforcePart1(self, policy_network, verbose=False):\n",
    "        ''' Improve policy by implementing vanilla version of Reinforce algo '''\n",
    "        \n",
    "        gamma = self.gamma\n",
    "        arr_objective = []\n",
    "        \n",
    "        for i in range(self.N): #improving policy for \"self.N\" number of iterations\n",
    "            n, objective = 0, 0\n",
    "            \n",
    "            # Resetting variables\n",
    "            self.num_steps = 0\n",
    "            self.max_steps_reached = False\n",
    "            \n",
    "            # Looping until a maximum number of steps are taken\n",
    "            while not(self.max_steps_reached):\n",
    "                # Run an episode with policy and count number of steps taken\n",
    "                r, log_prob_of_actions = self.runEpisode(policy_network)\n",
    "                n += 1 #increasing the episode count by 1\n",
    "                # Calculate discounted return (G_tau) and summation of log probs of actions from the episode\n",
    "                G_tau = sum([ r[t] * gamma**t for t in range(len(r))])\n",
    "                sum_lpa = sum(log_prob_of_actions)\n",
    "                objective += G_tau * sum_lpa\n",
    "                assert isinstance(objective, torch.Tensor)\n",
    "\n",
    "#             if verbose:\n",
    "#                 print('objective, n = ', objective.item(), n)\n",
    "\n",
    "            policy_network.optimizer.zero_grad()\n",
    "            objective = -objective/n #averaging objective over n episodes and flipping sign so it does gradient ascent\n",
    "            objective.backward()\n",
    "            policy_network.optimizer.step()\n",
    "            arr_objective.append(objective.item())\n",
    "\n",
    "            if verbose and (i%20 == 0):\n",
    "                print('Iteration: {0} \\t Objective: {1}'.format(i, objective))\n",
    "\n",
    "        return policy_network, arr_objective\n",
    "    \n",
    "    def doReinforcePart2(self, policy_network, verbose=False):\n",
    "        ''' Improve policy by implementing an imporved version of Reinforce algo '''\n",
    "        \n",
    "        gamma = self.gamma\n",
    "        arr_objective = []\n",
    "        \n",
    "        for i in range(self.N): #improving policy for \"self.N\" number of iterations\n",
    "            n, objective = 0, 0\n",
    "            \n",
    "            # Resetting variables\n",
    "            self.num_steps = 0\n",
    "            self.max_steps_reached = False\n",
    "            \n",
    "            # Looping until a maximum number of steps are taken\n",
    "            while not(self.max_steps_reached):\n",
    "                # Run an episode with policy and count number of steps taken\n",
    "                r, log_prob_of_actions = self.runEpisode(policy_network)\n",
    "                n += 1 #increasing the episode count by 1\n",
    "                \n",
    "                # Calculate discounted return (G_tau) and summation of log probs of actions from the episode\n",
    "                T = len(r)\n",
    "                for t in range(T):\n",
    "                    future_discounted_return = 0\n",
    "                    for t_prime in range(t, T):\n",
    "                        future_discounted_return += r[t_prime] * gamma**(t_prime - t)\n",
    "                    objective += log_prob_of_actions[t]*future_discounted_return\n",
    "\n",
    "                assert isinstance(objective, torch.Tensor)\n",
    "\n",
    "            policy_network.optimizer.zero_grad()\n",
    "            objective = -objective/n #averaging objective over n episodes and flipping sign so it does gradient ascent\n",
    "            objective.backward()\n",
    "            policy_network.optimizer.step()\n",
    "            arr_objective.append(objective.item())\n",
    "\n",
    "            if verbose and (i%20 == 0):\n",
    "                print('Iteration: {0} \\t Objective: {1}'.format(i, objective))\n",
    "\n",
    "        return policy_network, arr_objective\n",
    "\n",
    "    def doReinforcePart3(self, policy_network, verbose=False):\n",
    "        ''' Improve policy by implementing an imporved version of Reinforce algo '''\n",
    "        \n",
    "        gamma = self.gamma\n",
    "        arr_objective = []\n",
    "        \n",
    "        for i in range(self.N): #improving policy for \"self.N\" number of iterations\n",
    "            n, objective = 0, 0\n",
    "            \n",
    "            # Resetting variables\n",
    "            self.num_steps = 0\n",
    "            self.max_steps_reached = False\n",
    "            \n",
    "            # Looping until a maximum number of steps are taken\n",
    "            while not(self.max_steps_reached):\n",
    "                # Run an episode with policy and count number of steps taken\n",
    "                r, log_prob_of_actions = self.runEpisode(policy_network)\n",
    "                n += 1 #increasing the episode count by 1\n",
    "                \n",
    "                # Calculate discounted return (G_tau) and summation of log probs of actions from the episode\n",
    "                T = len(r)\n",
    "                b = mean(r)\n",
    "                for t in range(T):\n",
    "                    future_discounted_return = 0\n",
    "                    for t_prime in range(t, T):\n",
    "                        future_discounted_return += r[t_prime]*gamma**(t_prime - t) - b\n",
    "                    objective += log_prob_of_actions[t]*future_discounted_return\n",
    "\n",
    "                assert isinstance(objective, torch.Tensor)\n",
    "\n",
    "            policy_network.optimizer.zero_grad()\n",
    "            objective = -objective/n #averaging objective over n episodes and flipping sign so it does gradient ascent\n",
    "            objective.backward()\n",
    "            policy_network.optimizer.step()\n",
    "            arr_objective.append(objective.item())\n",
    "\n",
    "            if verbose and (i%20 == 0):\n",
    "                print('Iteration: {0} \\t Objective: {1}'.format(i, objective))\n",
    "\n",
    "        return policy_network, arr_objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Need to calculate $ J(\\theta) \\approx 1/N \\sum_{i=0}^{N} G(\\tau_{i}) \\sum_{t=0}^{T} log\\pi_{\\theta}(a_{t} | s_{t}) $\n",
    "## $ G(\\tau_{i}) = \\sum_{t=0}^{T} \\gamma^{t}r_{t}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    render_env = True\n",
    "    plot = True\n",
    "    \n",
    "    ## Part 1 (vanilla REINFORCE)\n",
    "    policy_network = makePolicyNN(num_actions=2, lr=1e-3)\n",
    "    pg = PolicyGradient(gamma=0.99, N = 200)\n",
    "    policy_network, arr_objective = pg.doReinforcePart1(policy_network, verbose=True)\n",
    "    \n",
    "#     ## Part 2 (future discounted returns) \n",
    "#     policy_network = makePolicyNN(num_actions=2, lr=1e-4)\n",
    "#     pg = PolicyGradient(gamma=0.9, N = 200, max_steps=500)\n",
    "#     policy_network, arr_objective = pg.doReinforcePart2(policy_network, verbose=True)\n",
    "    \n",
    "#     ## Part 3 (future discounted returns - bias) \n",
    "#     policy_network = makePolicyNN(num_actions=2, lr=1e-4)\n",
    "#     pg = PolicyGradient(gamma=0.9, N = 200, max_steps=500)\n",
    "#     policy_network, arr_objective = pg.doReinforcePart3(policy_network, verbose=True)\n",
    "    if plot:\n",
    "        fig, ax = plt.subplots(figsize=(11,7))\n",
    "        ax.plot(arr_objective)\n",
    "        ax.set_xlabel('Iteration')\n",
    "        ax.set_ylabel('Objective')\n",
    "        ax.set_title(' Objective vs iterations ')\n",
    "        plt.show()\n",
    "    \n",
    "    if render_env:\n",
    "        steps = 0\n",
    "        state = pg.env.reset()\n",
    "        done = False\n",
    "        while steps<200:\n",
    "            a, _ = pg.getAction(policy_network, state)\n",
    "            state, r, done, info = pg.env.step(a)\n",
    "            steps+=1\n",
    "            pg.env.render()\n",
    "        pg.env.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ece276c_venv",
   "language": "python",
   "name": "ece276c_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
