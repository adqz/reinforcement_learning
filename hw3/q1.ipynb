{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions.categorical import Categorical\n",
    "\n",
    "np.set_printoptions(suppress=True, precision=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(Policy, self).__init__()\n",
    "        self.fc1 = nn.Linear(4, 16)\n",
    "        self.fc2 = nn.Linear(16, 16)\n",
    "        self.fc3 = nn.Linear(16, num_classes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.softmax = nn.Softmax(dim=0) #important param to set --> dim\n",
    "        \n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.softmax(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "def makePolicyNN(num_actions):\n",
    "    ''' Initialize the policy class '''\n",
    "    assert isinstance(num_actions, int) and num_actions>0\n",
    "    \n",
    "    return Policy(num_actions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyGradient:\n",
    "    def __init__(self, gamma, N, T):\n",
    "        ''' Initialize the cart-pole environment '''\n",
    "        assert isinstance(gamma, float) and 0.0<gamma<1.0, 'Invalid gamma'\n",
    "        assert isinstance(N, int) and N>0\n",
    "        assert isinstance(T, int) and T>0\n",
    "        \n",
    "        self.env = gym.make (\"CartPole-v1\")\n",
    "        self.gamma = gamma\n",
    "        self.N = N\n",
    "        self.T = T\n",
    "        self.numActions = self.env.action_space.n\n",
    "    \n",
    "    def getAction(self, policy_network, state):\n",
    "        ''' Return an action from a stochastic policy '''\n",
    "        assert isinstance(state, np.ndarray) and len(state) == 4\n",
    "        \n",
    "        state = torch.from_numpy(state).float()\n",
    "        probs = policy_network(state)\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample() #type tensor\n",
    "        log_prob_of_action = m.log_prob(action)\n",
    "        \n",
    "        assert isinstance(action, int) and action in [0,1]\n",
    "        \n",
    "        return action.item(), log_prob_of_action.item()\n",
    "    \n",
    "    def generateEpisode(self, policy):\n",
    "        ''' Generate [s_t, a_t, r_t] pairs for one episode '''\n",
    "        \n",
    "        initial_state = self.env.reset()\n",
    "        initial_action, log_prob_a = self.getAction(policy_network, initial_state)\n",
    "        \n",
    "        states = [initial_state]\n",
    "        actions = [initial_action]\n",
    "        rewards = [r]\n",
    "        log_prob_of_actions = [log_prob_a]\n",
    "        \n",
    "        for t in range(self.T):\n",
    "            a, log_prob_a = self.getAction(policy_network, states[-1])\n",
    "            next_state, r, done, info = self.env.step(a)\n",
    "            \n",
    "            # Save next state and the action that led to it\n",
    "            states.append(next_state)\n",
    "            actions.append(a)\n",
    "            rewards.append(r)\n",
    "            log_prob_of_actions.append(log_prob_a)\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        assert len(states) - len(actions) == 0, 'Number of actions should be equal to number of states'\n",
    "        assert len(states) - len(rewards) == 0, 'Number of rewards should be equal to number of states'\n",
    "\n",
    "        episode = (states, actions, rewards, log_prob_of_actions)\n",
    "        return episode\n",
    "    \n",
    "    def calculateTerms(self, episode):\n",
    "        ''' Calculate terms of the objective function '''\n",
    "        assert isinstance(episode, tuple) and len(episode) == 4\n",
    "        gamma = self.gamma\n",
    "        rewards, log_prob_of_actions = episode[1], episode[3]\n",
    "        term1 = np.sum([rewards[t]*(gamma**t) for t in range(len(rewards))]) # calculating G(tau) which is Monte Carlo estimate of discounted return\n",
    "        term2 = np.sum(log_prob_of_actions)\n",
    "        \n",
    "        return term1, term2\n",
    "    \n",
    "    def calculateObjective(self, policy):\n",
    "        ''' Calculate J(theta) '''\n",
    "        \n",
    "        objective = 0\n",
    "        for i in range(self.N):\n",
    "            episode_i = self.generateEpisode(policy)\n",
    "            term1, term2 = self.calculateTerms(episode_i)\n",
    "            objective += term1*term2\n",
    "        \n",
    "        return objective/self.N\n",
    "    \n",
    "    def doVanillaReinforce(self, policy, max_iter):\n",
    "        ''' Improve policy by implementing vanilla version of Reinforce algo '''\n",
    "        for i in range(max_iter):\n",
    "            objective = self.calculateObjective(policy)\n",
    "            objective = torch.Tensor(objective)\n",
    "            objective.backward() #calculate gradient\n",
    "            policy_network.optimizer.step() #take a step in the direction of gradient\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Need to calculate $ J(\\theta) \\approx \\sum_{i=0}^{N} G(\\tau_{i}) \\sum_{t=0}^{T} log\\pi_{\\theta}(a_{t} | s_{t}) $\n",
    "## Term 1 = $ G(\\tau_{i}) = \\sum_{t=0}^{T} r_{t}$\n",
    "## Term 2 = $ \\sum_{t=0}^{T} log\\pi_{\\theta}(a_{t} | s_{t}) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_prob_of_action =  tensor(-0.6722, grad_fn=<SqueezeBackward1>)\n",
      "log =  0.0\n",
      "log_prob_of_action =  tensor(-0.6716, grad_fn=<SqueezeBackward1>)\n",
      "log =  0.0\n",
      "log_prob_of_action =  tensor(-0.6726, grad_fn=<SqueezeBackward1>)\n",
      "log =  0.0\n",
      "log_prob_of_action =  tensor(-0.6737, grad_fn=<SqueezeBackward1>)\n",
      "log =  0.0\n",
      "log_prob_of_action =  tensor(-0.7109, grad_fn=<SqueezeBackward1>)\n",
      "log =  -inf\n",
      "log_prob_of_action =  tensor(-0.7128, grad_fn=<SqueezeBackward1>)\n",
      "log =  -inf\n",
      "log_prob_of_action =  tensor(-0.7119, grad_fn=<SqueezeBackward1>)\n",
      "log =  -inf\n",
      "log_prob_of_action =  tensor(-0.7117, grad_fn=<SqueezeBackward1>)\n",
      "log =  -inf\n",
      "log_prob_of_action =  tensor(-0.7110, grad_fn=<SqueezeBackward1>)\n",
      "log =  -inf\n",
      "log_prob_of_action =  tensor(-0.6761, grad_fn=<SqueezeBackward1>)\n",
      "log =  0.0\n",
      "log_prob_of_action =  tensor(-0.7107, grad_fn=<SqueezeBackward1>)\n",
      "log =  -inf\n",
      "log_prob_of_action =  tensor(-0.6764, grad_fn=<SqueezeBackward1>)\n",
      "log =  0.0\n",
      "log_prob_of_action =  tensor(-0.7103, grad_fn=<SqueezeBackward1>)\n",
      "log =  -inf\n",
      "log_prob_of_action =  tensor(-0.6768, grad_fn=<SqueezeBackward1>)\n",
      "log =  0.0\n",
      "log_prob_of_action =  tensor(-0.6767, grad_fn=<SqueezeBackward1>)\n",
      "log =  0.0\n",
      "log_prob_of_action =  tensor(-0.7084, grad_fn=<SqueezeBackward1>)\n",
      "log =  -inf\n",
      "log_prob_of_action =  tensor(-0.7092, grad_fn=<SqueezeBackward1>)\n",
      "log =  -inf\n",
      "log_prob_of_action =  tensor(-0.6779, grad_fn=<SqueezeBackward1>)\n",
      "log =  0.0\n",
      "log_prob_of_action =  tensor(-0.6781, grad_fn=<SqueezeBackward1>)\n",
      "log =  0.0\n",
      "log_prob_of_action =  tensor(-0.6795, grad_fn=<SqueezeBackward1>)\n",
      "log =  0.0\n",
      "log_prob_of_action =  tensor(-0.7087, grad_fn=<SqueezeBackward1>)\n",
      "log =  -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adnanshahpurwala/anaconda3/envs/ece276c_venv/lib/python3.6/site-packages/ipykernel_launcher.py:20: RuntimeWarning: divide by zero encountered in log\n"
     ]
    }
   ],
   "source": [
    "policy_network = makePolicyNN(num_actions = 2)\n",
    "policy_network = policy_network\n",
    "\n",
    "pg = PolicyGradient(gamma=0.9)\n",
    "episode = pg.generateEpisode(policy_network, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "episode[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = gym.make('CartPole-v1')\n",
    "# env.reset()\n",
    "# env.action_space.sample()\n",
    "# state, r, done, info = env.step(0)\n",
    "# q = torch.from_numpy(state).float()\n",
    "\n",
    "# policy_network(q)\n",
    "# print(type(policy_network))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ece276c_venv",
   "language": "python",
   "name": "ece276c_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
