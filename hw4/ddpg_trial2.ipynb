{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import copy\n",
    "import shutil\n",
    "import argparse\n",
    "from argparse import Namespace\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "\n",
    "import gym\n",
    "import pybullet\n",
    "import pybulletgym.envs\n",
    "\n",
    "from collections import deque\n",
    "from operator import itemgetter\n",
    "from statistics import mean\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "parser = argparse.ArgumentParser(description='DDPG')\n",
    "parser.add_argument('-s', '--seed', type=int, help='seed for random initializations')\n",
    "parser.add_argument('-i', '--index', type=int, help='index of run')\n",
    "\n",
    "def weighSync(target_model, source_model, tau=0.001):\n",
    "    ''' A function to soft update target networks '''\n",
    "    assert isinstance(tau, float) and tau>0\n",
    "\n",
    "    for param_target, param_source in zip(target_model.parameters(), source_model.parameters()):\n",
    "        # Wrap in torch.no_grad() because weights have requires_grad=True, \n",
    "        # but we don't need to track this in autograd\n",
    "        with torch.no_grad():\n",
    "            param_target = tau*param_source + (1-tau)*param_target\n",
    "    \n",
    "    return target_model, source_model\n",
    "\n",
    "\n",
    "class Replay():\n",
    "    def __init__(self, buffer_size, init_length, state_dim, action_dim, env):\n",
    "        \"\"\"\n",
    "        A function to initialize the replay buffer.\n",
    "\n",
    "        param: init_length : Initial number of transitions to collect\n",
    "        param: state_dim : Size of the state space\n",
    "        param: action_dim : Size of the action space\n",
    "        param: env : gym environment object\n",
    "        \"\"\"\n",
    "        assert isinstance(buffer_size, int) and buffer_size>0\n",
    "        assert isinstance(init_length, int) and init_length>0\n",
    "        assert isinstance(state_dim, int) and state_dim>0\n",
    "        assert isinstance(action_dim, int) and action_dim>0\n",
    "        \n",
    "        self.buffer_size = buffer_size\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.buffer = deque()\n",
    "        \n",
    "        s = env.reset()\n",
    "        for i in range(init_length):\n",
    "            a = env.action_space.sample()\n",
    "            s_prime, r, done, _ = env.step(a)\n",
    "            self.buffer.append((s,a,r,s_prime))\n",
    "            s = s_prime\n",
    "            \n",
    "    def __len__(self):\n",
    "        ''' Return number of elements in buffer'''\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def buffer_add(self, exp):\n",
    "        \"\"\"\n",
    "        A function to add a dictionary to the buffer\n",
    "        param: exp : A tuple consisting of (state, action, reward, next state) in that order\n",
    "        \"\"\"\n",
    "        assert isinstance(exp, tuple) and len(exp) == 4\n",
    "        \n",
    "        if len(self) < self.buffer_size:\n",
    "            self.buffer.append(exp)\n",
    "        else:\n",
    "            self.buffer.popleft()\n",
    "            self.buffer.append(exp)\n",
    "\n",
    "    def buffer_sample(self, N):\n",
    "        \"\"\"\n",
    "        A function to sample N points from the buffer\n",
    "        param: N : Number of samples to obtain from the buffer\n",
    "        \"\"\"\n",
    "        assert isinstance(N, int) and N>0\n",
    "        indices = list(np.random.randint(low=0, high=len(self), size=N, dtype='int'))\n",
    "        sample = itemgetter(*indices)(self.buffer)\n",
    "        \n",
    "        assert len(sample) == N, 'You fucked up sampling bruh'\n",
    "        return sample\n",
    "\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        \"\"\"\n",
    "        Initialize the network\n",
    "        param: state_dim : Size of the state space\n",
    "        param: action_dim: Size of the action space\n",
    "        \"\"\"\n",
    "        assert isinstance(state_dim, int) and state_dim>0\n",
    "        assert isinstance(action_dim, int) and action_dim>0\n",
    "        \n",
    "        super(Actor, self).__init__()\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        \n",
    "        # NN layers and activations\n",
    "        self.fc1 = nn.Linear(state_dim, 400)\n",
    "        self.hidden1 = nn.Linear(400, 300)\n",
    "        self.fc2 = nn.Linear(300, action_dim)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"\n",
    "        Define the forward pass\n",
    "        param: state: The state of the environment\n",
    "        \"\"\"\n",
    "        \n",
    "        x = state\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.hidden1(x))\n",
    "        x = self.tanh(self.fc2(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        \"\"\"\n",
    "        Initialize the critic\n",
    "        param: state_dim : Size of the state space\n",
    "        param: action_dim : Size of the action space\n",
    "        \"\"\"\n",
    "        assert isinstance(state_dim, int) and state_dim>0\n",
    "        assert isinstance(action_dim, int) and action_dim>0\n",
    "        \n",
    "        super(Critic, self).__init__()\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        \n",
    "        # NN layers and activations\n",
    "        self.fc1 = nn.Linear(state_dim + action_dim, 400)\n",
    "        self.hidden1 = nn.Linear(400, 300)\n",
    "        self.fc2 = nn.Linear(300, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        \"\"\" Define the forward pass of the critic \"\"\"\n",
    "        assert state.shape[1] == self.state_dim, 'state must be of dim (batch_size, %d)'%self.state_dim\n",
    "        assert action.shape[1] == self.action_dim, 'action must be of dim (batch_size, %d)'%self.action_dim\n",
    "        \n",
    "        x = torch.cat((state, action), dim=1) #concatenating to form input\n",
    "        \n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.hidden1(x))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class DDPG():\n",
    "    def __init__(\n",
    "            self,\n",
    "            env,\n",
    "            test_env,\n",
    "            state_dim,    \n",
    "            action_dim,\n",
    "            critic_lr=3e-4,\n",
    "            actor_lr=3e-4,\n",
    "            gamma=0.99,\n",
    "            batch_size=100,\n",
    "            ev_n_steps=100,\n",
    "            verbose=False\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Implementing DPPG algorithm from paper - Continuous control with deep reinforcement learning\n",
    "        link - https://arxiv.org/pdf/1509.02971.pdf\n",
    "        \n",
    "        param: env: An gym environment\n",
    "        param: action_dim: Size of action space\n",
    "        param: state_dim: Size of state space\n",
    "        param: critic_lr: Learning rate of the critic\n",
    "        param: actor_lr: Learning rate of the actor\n",
    "        param: gamma: The discount factor\n",
    "        param: batch_size: The batch size for training\n",
    "        \"\"\"\n",
    "\n",
    "        assert isinstance(state_dim, int) and state_dim>0\n",
    "        assert isinstance(action_dim, int) and action_dim>0\n",
    "        assert isinstance(batch_size, int) and batch_size>0\n",
    "        assert isinstance(critic_lr, (int, float)) and critic_lr>0\n",
    "        assert isinstance(actor_lr, (int, float)) and actor_lr>0\n",
    "        assert isinstance(gamma, (int, float)) and gamma>0\n",
    "        assert isinstance(ev_n_steps, (int, float)) and ev_n_steps>0\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.action_dim = action_dim\n",
    "        self.state_dim = state_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.ev_n_steps = ev_n_steps\n",
    "        self.env = env\n",
    "        self.test_env = test_env\n",
    "        self.num_episodes = 0\n",
    "        self.avg_rewards = []\n",
    "        self.obj_actor = []\n",
    "        self.loss_critic = []\n",
    "\n",
    "        # Create a actor and actor_target with same initial weights\n",
    "        self.actor = Actor(state_dim, action_dim)\n",
    "        self.actor = self.init_weights(self.actor) #initialize weights according to paper\n",
    "        self.actor_target = copy.deepcopy(self.actor) #both networks have the same initial weights \n",
    "\n",
    "        # Create a critic and critic_target with same initial weights\n",
    "        self.critic = Critic(state_dim, action_dim)\n",
    "        self.critic = self.init_weights(self.critic) #initialize weights according to paper\n",
    "        self.critic_target = copy.deepcopy(self.critic) #both networks have the same initial weights \n",
    "\n",
    "        # Define optimizer for actor and critic\n",
    "        self.optimizer_actor = optim.Adam(self.actor.parameters(), lr=actor_lr)\n",
    "        self.optimizer_critic = optim.Adam(self.critic.parameters(), lr=critic_lr)\n",
    "        self.mse_loss = nn.MSELoss(reduction='mean')\n",
    "\n",
    "        # Define a replay buffer\n",
    "        self.ReplayBuffer = Replay(10000, 1000, state_dim, action_dim, self.env)\n",
    "    \n",
    "    def init_weights(self, network):\n",
    "        '''\n",
    "        Initialize weights (as mentioned in paper) from a uniform distribution \n",
    "        based on the fan-in of the layer\n",
    "        \n",
    "        WARNING: Will only work if each layer is fully connected\n",
    "        '''\n",
    "        \n",
    "        for name, param in network.named_parameters():\n",
    "            if 'bias' in name: #if bias param, use f of the same layer\n",
    "                f = last_f\n",
    "            else:\n",
    "                f = param.shape[1] #picking 2nd dim = number of inputs for that layer\n",
    "                last_f = f\n",
    "            # Initialize weights by sampling from uniform dist.\n",
    "            assert isinstance(f, int) and f>0, 'fan in must be int and greater than 0'\n",
    "            nn.init.uniform_(param.data, a = -1/np.sqrt(f), b = 1/np.sqrt(f))\n",
    "        \n",
    "        return network\n",
    "    \n",
    "    def save_actor(self, index):\n",
    "        ''' Saves the policy NN'''\n",
    "        filename = 'q1_policy_' + str(index) + '.pth.tar'\n",
    "        state = {'state_dict': self.actor.state_dict(),\n",
    "                 'optimizer': self.optimizer_actor.state_dict(),\n",
    "                 'seed': args.seed }\n",
    "        torch.save(state, filename)\n",
    "        torch.save(self.best_actor_state, 'best_'+filename)\n",
    "    \n",
    "    def update_target_networks(self):\n",
    "        \"\"\"\n",
    "        A function to update the target networks\n",
    "        \"\"\"\n",
    "        self.actor_target, self.actor = weighSync(self.actor_target, self.actor)\n",
    "        self.critic_target, self.critic = weighSync(self.critic_target, self.critic)\n",
    "    \n",
    "    def getAverageReward(self):\n",
    "        ''' Run the policy and return average reward '''\n",
    "        rewards = []\n",
    "        s = self.test_env.reset()\n",
    "        done = False\n",
    "        while not(done):\n",
    "            s = torch.from_numpy(s).float().unsqueeze(dim=1).T\n",
    "            a = self.actor(s).detach().squeeze().numpy()\n",
    "            s, r, done, _ = self.test_env.step(a)\n",
    "            rewards.append(r)\n",
    "        \n",
    "        avg_reward = sum(rewards)\n",
    "        assert isinstance(avg_reward, (int, float))\n",
    "        \n",
    "        return avg_reward\n",
    "\n",
    "    def train(self, max_num_steps):\n",
    "        \"\"\"\n",
    "        Train the policy for the given number of iterations\n",
    "        :param num_steps:The number of steps to train the policy for\n",
    "        \"\"\"\n",
    "        assert isinstance(max_num_steps, int) and max_num_steps>0\n",
    "        \n",
    "        gamma = self.gamma\n",
    "        state = self.env.reset()\n",
    "        done = False\n",
    "        best_r = -np.inf\n",
    "        \n",
    "        # Training starts now\n",
    "        for num_steps in tqdm(range(max_num_steps)):\n",
    "            # Reset env when it reached terminal state\n",
    "            if done:\n",
    "                self.num_episodes += 1\n",
    "                state = self.env.reset()\n",
    "            \n",
    "            state_tensor = torch.from_numpy(state).float().unsqueeze(dim=1).T\n",
    "            action = self.actor(state_tensor)\n",
    "            \n",
    "            # Sampling from the nD Gaussian\n",
    "            m = MultivariateNormal(action, torch.eye(self.action_dim)*0.1)\n",
    "            action = m.sample()\n",
    "            action = action.detach().squeeze().numpy()\n",
    "            \n",
    "            state_next, rewd, done, _ = self.env.step(action)\n",
    "            \n",
    "            # Storing transition in buffer\n",
    "            self.ReplayBuffer.buffer_add((state, action, rewd, state_next))\n",
    "            state = state_next\n",
    "\n",
    "            # Sampling N points from buffer\n",
    "            minibatch = self.ReplayBuffer.buffer_sample(N = self.batch_size)\n",
    "            \n",
    "            # Operating on minibatch\n",
    "            s = torch.Tensor([el[0] for el in minibatch])                        #dim (batch_size, state_dim)\n",
    "            a = torch.Tensor([el[1] for el in minibatch])                        #dim (batch_size, action_dim)\n",
    "            r = torch.Tensor([el[2] for el in minibatch]).unsqueeze(dim=1)       #dim (batch_size, 1)\n",
    "            s_prime = torch.Tensor([el[3] for el in minibatch])                  #dim (batch_size, state_dim)\n",
    "            \n",
    "            a_target = self.actor_target(s_prime)                            #dim(batch_size, action_dim)\n",
    "            y_i = r + gamma*self.critic_target(s_prime, a_target)            #dim (batch_size, 1)\n",
    "            loss_critic = self.mse_loss(y_i, self.critic(s,a)) #mse loss     #dim (batch_size, 1)\n",
    "            \n",
    "            a = self.actor(s)\n",
    "            obj_actor = (self.critic(s, a).mean()) * -1.0\n",
    "            \n",
    "            \n",
    "            # Update critic\n",
    "            loss_critic.backward()\n",
    "            self.optimizer_critic.step()\n",
    "            \n",
    "            # Update actor\n",
    "            obj_actor.backward()\n",
    "            self.optimizer_actor.step()\n",
    "            \n",
    "            # Update target networks\n",
    "            self.update_target_networks()\n",
    "            \n",
    "            # Store losses\n",
    "            self.obj_actor.append(obj_actor.item())\n",
    "            self.loss_critic.append(loss_critic.item())\n",
    "\n",
    "            # Zero gradients of optimizer\n",
    "            self.optimizer_critic.zero_grad()    \n",
    "            self.optimizer_actor.zero_grad()\n",
    "            \n",
    "            if num_steps%self.ev_n_steps == 0:\n",
    "                r = self.getAverageReward()\n",
    "                # Saving best actor model till now\n",
    "                is_best = r > best_r\n",
    "                best_r = max(r, best_r)\n",
    "                if is_best:\n",
    "                    self.best_actor_state = {'state_dict': self.actor.state_dict(),\n",
    "                                             'optimizer': self.optimizer_actor.state_dict(),\n",
    "                                             'avg_reward': r,\n",
    "                                             'obj_actor': obj_actor.item(),\n",
    "                                             'loss_critic': loss_critic.item(),\n",
    "                                             'seed': args.seed\n",
    "                                            }\n",
    "                \n",
    "                self.avg_rewards.append(r)\n",
    "                if verbose:\n",
    "                    print('Num steps: {0} \\t Avg Reward: {1:.3f} \\t Obj(Actor): {2:.3f} \\t Loss(Critic): {3:.3f} \\t Num eps: {4}'\n",
    "                          .format(num_steps, r, obj_actor.item(), loss_critic.item(), self.num_episodes))\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_dir=/Users/adnanshahpurwala/anaconda3/envs/ece276c_venv/lib/python3.6/site-packages/pybullet_envs/bullet\n",
      "options= \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/100000 [00:00<2:54:48,  9.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "options= \n",
      "Num steps: 0 \t Avg Reward: -23.407 \t Obj(Actor): 0.311 \t Loss(Critic): 0.090 \t Num eps: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 94/100000 [00:03<1:00:43, 27.42it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-192b17d127fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m     )\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m# Train the policy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mddpg_object\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1e5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;31m# Save actor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-b8f26602658e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, max_num_steps)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m             \u001b[0;31m# Update critic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m             \u001b[0mloss_critic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer_critic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ece276c_venv/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    148\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \"\"\"\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ece276c_venv/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    global args\n",
    "#     args = parser.parse_args()\n",
    "    args = Namespace(seed= 4, index= 9)\n",
    "    # Seed value\n",
    "    torch.manual_seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    \n",
    "    # Define the environment\n",
    "    env = gym.make(\"modified_gym_env:ReacherPyBulletEnv-v1\", rand_init=True)\n",
    "    test_env = gym.make(\"modified_gym_env:ReacherPyBulletEnv-v1\", rand_init=False)\n",
    "    plot = True\n",
    "    verbose = True\n",
    "    \n",
    "    # Define Deep Deterministic Policy Gradient object\n",
    "    ddpg_object = DDPG(\n",
    "        env,\n",
    "        test_env,\n",
    "        8,\n",
    "        2,\n",
    "        critic_lr=1e-3,\n",
    "        actor_lr=1e-4,\n",
    "        gamma=0.99,\n",
    "        batch_size=500,\n",
    "        ev_n_steps=300, #evaluate every n steps\n",
    "        verbose=verbose\n",
    "    )\n",
    "    # Train the policy\n",
    "    ddpg_object.train(int(1e5))\n",
    "    \n",
    "    # Save actor\n",
    "    ddpg_object.save_actor(args.index)\n",
    "    \n",
    "    if plot:\n",
    "        fig, axs = plt.subplots(3,1, figsize=(10,15))\n",
    "        axs = axs.flatten()\n",
    "        axs[0].plot(ddpg_object.avg_rewards)\n",
    "        axs[0].set_ylabel('Avg Rewards')\n",
    "        axs[0].set_xlabel('Every %d iterations'%ddpg_object.ev_n_steps)\n",
    "        axs[0].set_title(' Avg Rewards vs iterations ')\n",
    "    #     axs[1].plot(ddpg_object.obj_actor[::ddpg_object.ev_n_steps])\n",
    "        axs[1].plot(ddpg_object.obj_actor)\n",
    "        axs[1].set_ylabel('Actor Obj')\n",
    "        axs[1].set_title(' Actor Obj vs iterations ')\n",
    "    #     axs[2].plot(ddpg_object.loss_critic[::ddpg_object.ev_n_steps])\n",
    "        axs[2].plot(ddpg_object.loss_critic)\n",
    "        axs[2].set_ylabel('Critic Loss')\n",
    "        axs[2].set_title(' Critic Loss vs iterations ')\n",
    "        plt.savefig('index_'+str(args.index)+'.png', dpi=300)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ece276c_venv",
   "language": "python",
   "name": "ece276c_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
