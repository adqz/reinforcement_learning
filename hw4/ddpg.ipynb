{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Learn a policy using DDPG for the reach task\"\"\"\n",
    "import numpy as np\n",
    "import time\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "\n",
    "import gym\n",
    "import pybullet\n",
    "import pybulletgym.envs\n",
    "\n",
    "from collections import deque\n",
    "from operator import itemgetter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(1000)\n",
    "\n",
    "\n",
    "def weighSync(target_model, source_model, tau=0.001):\n",
    "    ''' A function to soft update target networks '''\n",
    "    assert isinstance(tau, float) and tau>0\n",
    "\n",
    "    for param_target, param_source in zip(target_model.parameters(), source_model.parameters()):\n",
    "        # Wrap in torch.no_grad() because weights have requires_grad=True, \n",
    "        # but we don't need to track this in autograd\n",
    "        with torch.no_grad():\n",
    "            param_target = tau*param_source + (1-tau)*param_target\n",
    "    \n",
    "    return target_model, source_model\n",
    "\n",
    "\n",
    "class Replay():\n",
    "    def __init__(self, buffer_size, init_length, state_dim, action_dim, env):\n",
    "        \"\"\"\n",
    "        A function to initialize the replay buffer.\n",
    "\n",
    "        param: init_length : Initial number of transitions to collect\n",
    "        param: state_dim : Size of the state space\n",
    "        param: action_dim : Size of the action space\n",
    "        param: env : gym environment object\n",
    "        \"\"\"\n",
    "        self.buffer_size = buffer_size\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        \n",
    "        self.buffer = deque() #list like object for which removing elements from left is faster\n",
    "        \n",
    "        s = env.reset()\n",
    "        for i in range(init_length):\n",
    "            a = env.action_space.sample()\n",
    "            s_prime, r, done, _ = env.step(a)\n",
    "            self.buffer.append({\n",
    "                's': s,\n",
    "                'a': a,\n",
    "                'r': r,\n",
    "                's_prime': s_prime\n",
    "            })\n",
    "    \n",
    "    def __len__(self):\n",
    "        ''' Return number of elements in buffer'''\n",
    "        return len(self.buffer)\n",
    "    \n",
    "    def buffer_add(self, exp):\n",
    "        \"\"\"\n",
    "        A function to add a dictionary to the buffer\n",
    "        param: exp : A dictionary consisting of state, action, reward , next state and done flag\n",
    "        \"\"\"\n",
    "        assert isinstance(exp, dict) and len(exp) == 4\n",
    "        assert len(self.buffer) <= self.buffer_size, 'Buffer size exceeded'\n",
    "        if len(self.buffer) < self.buffer_size:\n",
    "            self.buffer.append(exp)\n",
    "        else:\n",
    "            self.buffer.popleft() #removing the 1st element (left most element)\n",
    "            self.buffer.append(exp)\n",
    "\n",
    "    def buffer_sample(self, N):\n",
    "        \"\"\"\n",
    "        A function to sample N points from the buffer\n",
    "        param: N : Number of samples to obtain from the buffer\n",
    "        \"\"\"\n",
    "        indices = list(np.random.randint(low=0, high=len(self.buffer), size=N, dtype='int'))\n",
    "        sample = itemgetter(*indices)(self.buffer) #extarct values at indices from buffer\n",
    "        \n",
    "        return sample\n",
    "        \n",
    "\n",
    "class Actor(nn.Module):\n",
    "    #TODO: Complete the function\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        \"\"\"\n",
    "        Initialize the network\n",
    "        param: state_dim : Size of the state space\n",
    "        param: action_dim: Size of the action space\n",
    "        \"\"\"\n",
    "        assert isinstance(state_dim, int) and state_dim>0\n",
    "        assert isinstance(action_dim, int) and action_dim>0\n",
    "        \n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 400)\n",
    "        self.hidden1 = nn.Linear(400, 300)\n",
    "        self.fc2 = nn.Linear(300, action_dim)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    #TODO: Complete the function\n",
    "    def forward(self, state):\n",
    "        \"\"\"\n",
    "        Define the forward pass\n",
    "        param: state: The state of the environment\n",
    "        \"\"\"\n",
    "        state = self.relu(self.fc1(state))\n",
    "        state = self.relu(self.hidden(state))\n",
    "        state = self.tanh(self.fc2(state))\n",
    "\n",
    "        return state\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        \"\"\"\n",
    "        Initialize the critic\n",
    "        param: state_dim : Size of the state space\n",
    "        param: action_dim : Size of the action space\n",
    "        \"\"\"\n",
    "        assert isinstance(state_dim, int) and state_dim>0\n",
    "        assert isinstance(action_dim, int) and action_dim>0\n",
    "        \n",
    "        super(Critic, self).__init__()\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        \n",
    "        # NN layers and activations\n",
    "        self.fc1 = nn.Linear(state_dim + action_dim, 400)\n",
    "        self.hidden1 = nn.Linear(400, 300)\n",
    "        self.fc2 = nn.Linear(300, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        \"\"\" Define the forward pass of the critic \"\"\"\n",
    "        assert isinstance(state, np.ndarray)\n",
    "        assert isinstance(action, np.ndarray)\n",
    "        assert state.shape == (self.state_dim, ), 'state must be 1D and of size (%d,)'%self.state_dim\n",
    "        assert action.shape == (self.action_dim, ), 'action must be 1D and of size (%d,)'%self.action_dim\n",
    "        \n",
    "        state, action = state.from_numpy(), action.from_numpy() #numpy to torch tensor\n",
    "        x = torch.cat((state, action), dim=0) #concatenating to form input\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.hidden(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class DDPG():\n",
    "    def __init__(\n",
    "            self,\n",
    "            env,\n",
    "            action_dim,\n",
    "            state_dim,\n",
    "            critic_lr=3e-4,\n",
    "            actor_lr=3e-4,\n",
    "            gamma=0.99,\n",
    "            batch_size=100,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        param: env: An gym environment\n",
    "        param: action_dim: Size of action space\n",
    "        param: state_dim: Size of state space\n",
    "        param: critic_lr: Learning rate of the critic\n",
    "        param: actor_lr: Learning rate of the actor\n",
    "        param: gamma: The discount factor\n",
    "        param: batch_size: The batch size for training\n",
    "        \"\"\"\n",
    "        assert isinstance(state_dim, int) and state_dim>0\n",
    "        assert isinstance(action_dim, int) and action_dim>0\n",
    "        assert isinstance(batch_size, int) and batch_size>0\n",
    "        assert isinstance(critic_lr, (int, float)) and critic_lr>0\n",
    "        assert isinstance(actor_lr, (int, float)) and actor_lr>0\n",
    "        assert isinstance(gamma, (int, float)) and gamma>0\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.env = env\n",
    "\n",
    "        # Create a actor and actor_target with same initial weights\n",
    "        self.actor = Actor(state_dim, action_dim)\n",
    "        self.actor_target = copy.deepcopy(self.actor) #both networks have the same initial weights \n",
    "\n",
    "        # Create a critic and critic_target with same initial weights\n",
    "        self.critic = Critic(state_dim, action_dim)\n",
    "        self.critic_target = copy.deepcopy(self.critic) #both networks have the same initial weights \n",
    "\n",
    "        # Define optimizer for actor and critic\n",
    "        self.optimizer_actor = optim.Adam(self.actor.parameters(), lr=actor_lr)\n",
    "        self.optimizer_critic = optim.Adam(self.critic.parameters(), lr=critic_lr)\n",
    "\n",
    "        # Define a replay buffer\n",
    "        self.ReplayBuffer = Replay(10000, 1000, state_dim, action_dim, self.env)\n",
    "\n",
    "    def update_target_networks(self):\n",
    "        \"\"\"\n",
    "        A function to update the target networks\n",
    "        \"\"\"\n",
    "        weighSync(self.actor_target, self.actor)\n",
    "        weighSync(self.critic_target, self.critic)\n",
    "\n",
    "    # TODO: Complete the function\n",
    "    def update_network(self):\n",
    "        \"\"\"\n",
    "        A function to update the function just once\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    # TODO: Complete the function\n",
    "    def train(self, num_steps):\n",
    "        \"\"\"\n",
    "        Train the policy for the given number of iterations\n",
    "        :param num_steps:The number of steps to train the policy for\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "options= \n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Define the environment\n",
    "    env = gym.make(\"modified_gym_env:ReacherPyBulletEnv-v1\", rand_init=False)\n",
    "\n",
    "    ddpg_object = DDPG(\n",
    "        env,\n",
    "        8,\n",
    "        2,\n",
    "        critic_lr=1e-3,\n",
    "        actor_lr=1e-3,\n",
    "        gamma=0.99,\n",
    "        batch_size=100,\n",
    "    )\n",
    "    # # Train the policy\n",
    "    # ddpg_object.train(100)\n",
    "\n",
    "    # # Evaluate the final policy\n",
    "    # state = env.reset()\n",
    "    # done = False\n",
    "    # while not done:\n",
    "    #     action = ddpg_object.actor(state).detach().squeeze().numpy()\n",
    "    #     next_state, r, done, _ = env.step(action)\n",
    "    #     env.render()\n",
    "    #     time.sleep(0.1)\n",
    "    #     state = next_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ddpg_object.ReplayBuffer.buffer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ece276c_venv",
   "language": "python",
   "name": "ece276c_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
