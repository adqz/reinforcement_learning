{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Learn a policy using DDPG for the reach task\"\"\"\n",
    "import numpy as np\n",
    "import time\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "\n",
    "import gym\n",
    "import pybullet\n",
    "import pybulletgym.envs\n",
    "\n",
    "from collections import deque\n",
    "from operator import itemgetter\n",
    "from statistics import mean\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(1000)\n",
    "\n",
    "\n",
    "def weighSync(target_model, source_model, tau=0.001):\n",
    "    ''' A function to soft update target networks '''\n",
    "    assert isinstance(tau, float) and tau>0\n",
    "\n",
    "    for param_target, param_source in zip(target_model.parameters(), source_model.parameters()):\n",
    "        # Wrap in torch.no_grad() because weights have requires_grad=True, \n",
    "        # but we don't need to track this in autograd\n",
    "        with torch.no_grad():\n",
    "            param_target = tau*param_source + (1-tau)*param_target\n",
    "    \n",
    "    return target_model, source_model\n",
    "\n",
    "\n",
    "class Replay():\n",
    "    def __init__(self, buffer_size, init_length, state_dim, action_dim, env):\n",
    "        \"\"\"\n",
    "        A function to initialize the replay buffer.\n",
    "\n",
    "        param: init_length : Initial number of transitions to collect\n",
    "        param: state_dim : Size of the state space\n",
    "        param: action_dim : Size of the action space\n",
    "        param: env : gym environment object\n",
    "        \"\"\"\n",
    "        assert isinstance(buffer_size, int) and buffer_size>0\n",
    "        assert isinstance(init_length, int) and init_length>0\n",
    "        assert isinstance(state_dim, int) and state_dim>0\n",
    "        assert isinstance(action_dim, int) and action_dim>0\n",
    "        \n",
    "        self.buffer_size = buffer_size\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        \n",
    "        self.buffer = deque() #list like object for which removing elements from left is faster\n",
    "        \n",
    "        s = env.reset()\n",
    "        for i in range(init_length):\n",
    "            a = env.action_space.sample()\n",
    "            s_prime, r, done, _ = env.step(a)\n",
    "            self.buffer.append({\n",
    "                's': s,\n",
    "                'a': a,\n",
    "                'r': r,\n",
    "                's_prime': s_prime\n",
    "            })\n",
    "    \n",
    "    def __len__(self):\n",
    "        ''' Return number of elements in buffer'''\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def buffer_add(self, exp):\n",
    "        \"\"\"\n",
    "        A function to add a dictionary to the buffer\n",
    "        param: exp : A dictionary consisting of state, action, reward , next state and done flag\n",
    "        \"\"\"\n",
    "        assert isinstance(exp, dict) and len(exp) == 4\n",
    "        assert len(self.buffer) <= self.buffer_size, 'Buffer size exceeded. You fucked up'\n",
    "        \n",
    "        if len(self.buffer) < self.buffer_size:\n",
    "            self.buffer.append(exp)\n",
    "        else:\n",
    "            self.buffer.popleft() #removing the 1st element (left most element)\n",
    "            self.buffer.append(exp)\n",
    "\n",
    "    def buffer_sample(self, N):\n",
    "        \"\"\"\n",
    "        A function to sample N points from the buffer\n",
    "        param: N : Number of samples to obtain from the buffer\n",
    "        \"\"\"\n",
    "        assert isinstance(N, int) and N>0\n",
    "        indices = list(np.random.randint(low=0, high=len(self.buffer), size=N, dtype='int'))\n",
    "        sample = itemgetter(*indices)(self.buffer) #extarct values at indices from buffer\n",
    "        \n",
    "        return sample\n",
    "\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    #TODO: Complete the function\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        \"\"\"\n",
    "        Initialize the network\n",
    "        param: state_dim : Size of the state space\n",
    "        param: action_dim: Size of the action space\n",
    "        \"\"\"\n",
    "        assert isinstance(state_dim, int) and state_dim>0\n",
    "        assert isinstance(action_dim, int) and action_dim>0\n",
    "        \n",
    "        super(Actor, self).__init__()\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        \n",
    "        # NN layers and activations\n",
    "        self.fc1 = nn.Linear(state_dim, 400)\n",
    "        self.hidden1 = nn.Linear(400, 300)\n",
    "        self.fc2 = nn.Linear(300, action_dim)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"\n",
    "        Define the forward pass\n",
    "        param: state: The state of the environment\n",
    "        \"\"\"\n",
    "#         assert isinstance(state, torch.Tensor)\n",
    "        assert state.shape == (self.state_dim, ), 'state must be 1D and of size (%d,)'%self.state_dim\n",
    "        \n",
    "        if not(isinstance(state, torch.Tensor)):\n",
    "            state = torch.from_numpy(state).float()\n",
    "        \n",
    "        x = state\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.hidden1(x))\n",
    "        x = self.tanh(self.fc2(x))\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def getAction(self, state, add_noise_flag = False, noise = 0.1):\n",
    "        '''\n",
    "        Returns an action by doing a forward pass. If add_noise_flag is True, \n",
    "        action is sampled from a multivariate Normal distributio with stddev = noise and mean = output of net\n",
    "        \n",
    "        :rtype: np.ndarray\n",
    "        '''\n",
    "#         print('type(state), shape = ',type(state), state.shape)\n",
    "#         print('self.state_dim = ', self.state_dim)\n",
    "        assert isinstance(state, np.ndarray)\n",
    "        assert state.shape == (self.state_dim, )\n",
    "        assert isinstance(noise, (int, float)) and noise >=0\n",
    "        assert isinstance(add_noise_flag, bool)\n",
    "        \n",
    "        state = torch.from_numpy(state).float()\n",
    "        action = self.forward(state) #forward pass\n",
    "        \n",
    "        if add_noise_flag:\n",
    "            # Sampling from the nD Gaussian\n",
    "            m = MultivariateNormal(action, torch.eye(self.action_dim)*noise)\n",
    "            action = m.sample()\n",
    "        \n",
    "        return action.detach().squeeze().numpy()\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        \"\"\"\n",
    "        Initialize the critic\n",
    "        param: state_dim : Size of the state space\n",
    "        param: action_dim : Size of the action space\n",
    "        \"\"\"\n",
    "        assert isinstance(state_dim, int) and state_dim>0\n",
    "        assert isinstance(action_dim, int) and action_dim>0\n",
    "        \n",
    "        super(Critic, self).__init__()\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        \n",
    "        # NN layers and activations\n",
    "        self.fc1 = nn.Linear(state_dim + action_dim, 400)\n",
    "        self.hidden1 = nn.Linear(400, 300)\n",
    "        self.fc2 = nn.Linear(300, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        \"\"\" Define the forward pass of the critic \"\"\"\n",
    "        assert isinstance(state, np.ndarray)\n",
    "        assert isinstance(action, np.ndarray)\n",
    "        assert state.shape == (self.state_dim, ), 'state must be 1D and of size (%d,)'%self.state_dim\n",
    "        assert action.shape == (self.action_dim, ), 'action must be 1D and of size (%d,)'%self.action_dim\n",
    "        \n",
    "        state, action = torch.from_numpy(state).float(), torch.from_numpy(action).float() #numpy to torch tensor\n",
    "        x = torch.cat((state, action), dim=0) #concatenating to form input\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.hidden1(x))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class DDPG():\n",
    "    def __init__(\n",
    "            self,\n",
    "            env,\n",
    "            test_env,\n",
    "            state_dim,    \n",
    "            action_dim,\n",
    "            critic_lr=3e-4,\n",
    "            actor_lr=3e-4,\n",
    "            gamma=0.99,\n",
    "            batch_size=100,\n",
    "            ev_n_steps=100,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Implementing DPPG algorithm from paper - Continuous control with deep reinforcement learning\n",
    "        link - https://arxiv.org/pdf/1509.02971.pdf\n",
    "        \n",
    "        param: env: An gym environment\n",
    "        param: action_dim: Size of action space\n",
    "        param: state_dim: Size of state space\n",
    "        param: critic_lr: Learning rate of the critic\n",
    "        param: actor_lr: Learning rate of the actor\n",
    "        param: gamma: The discount factor\n",
    "        param: batch_size: The batch size for training\n",
    "        \"\"\"\n",
    "\n",
    "        assert isinstance(state_dim, int) and state_dim>0\n",
    "        assert isinstance(action_dim, int) and action_dim>0\n",
    "        assert isinstance(batch_size, int) and batch_size>0\n",
    "        assert isinstance(critic_lr, (int, float)) and critic_lr>0\n",
    "        assert isinstance(actor_lr, (int, float)) and actor_lr>0\n",
    "        assert isinstance(gamma, (int, float)) and gamma>0\n",
    "        assert isinstance(ev_n_steps, (int, float)) and ev_n_steps>0\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.ev_n_steps = ev_n_steps\n",
    "        self.env = env\n",
    "        self.test_env = test_env\n",
    "        self.num_episodes = 0\n",
    "        self.avg_rewards = []\n",
    "\n",
    "        # Create a actor and actor_target with same initial weights\n",
    "        self.actor = Actor(state_dim, action_dim)\n",
    "        self.actor = self.init_weights(self.actor) #initialize weights according to paper\n",
    "        self.actor_target = copy.deepcopy(self.actor) #both networks have the same initial weights \n",
    "\n",
    "        # Create a critic and critic_target with same initial weights\n",
    "        self.critic = Critic(state_dim, action_dim)\n",
    "        self.critic = self.init_weights(self.critic) #initialize weights according to paper\n",
    "        self.critic_target = copy.deepcopy(self.critic) #both networks have the same initial weights \n",
    "\n",
    "        # Define optimizer for actor and critic\n",
    "        self.optimizer_actor = optim.Adam(self.actor.parameters(), lr=actor_lr)\n",
    "        self.optimizer_critic = optim.Adam(self.critic.parameters(), lr=critic_lr)\n",
    "        self.loss_fn = nn.MSELoss(reduction='sum')\n",
    "\n",
    "        # Define a replay buffer\n",
    "        self.ReplayBuffer = Replay(10000, 1000, state_dim, action_dim, self.env)\n",
    "    \n",
    "    def init_weights(self, network):\n",
    "        '''\n",
    "        Initialize weights (as mentioned in paper) from a uniform distribution \n",
    "        based on the fan-in of the layer\n",
    "        \n",
    "        WARNING: Will only work if each layer is fully connected\n",
    "        '''\n",
    "        \n",
    "        for name, param in network.named_parameters():\n",
    "            if 'bias' in name: #if bias param, use f of the same layer\n",
    "                f = last_f\n",
    "            else:\n",
    "                f = param.shape[1] #picking 2nd dim = number of inputs for that layer\n",
    "                last_f = f\n",
    "            # Initialize weights by sampling from uniform dist.\n",
    "            assert isinstance(f, int) and f>0, 'fan in must be int and greater than 0'\n",
    "            nn.init.uniform_(param.data, a = -1/np.sqrt(f), b = 1/np.sqrt(f))\n",
    "#             nn.init.uniform_(param.data, a = 0.9999, b = 1)\n",
    "        \n",
    "        return network\n",
    "    \n",
    "    def save_actor(self, index):\n",
    "        ''' Saves the policy NN'''\n",
    "        filename = 'q1_policy_' + str(index) + '.pth.tar'\n",
    "        state = { 'state_dict': self.actor.state_dict(),\n",
    "                 'optimizer': self.optimizer_actor.state_dict() }\n",
    "        torch.save(state, filename)\n",
    "        \n",
    "    def save_critic(self, index):\n",
    "        ''' Saves the critic NN'''\n",
    "        filename = 'q1_critic_' + str(index) + '.pth.tar'\n",
    "        state = { 'state_dict': self.critic.state_dict(),\n",
    "                 'optimizer': self.optimizer_critic.state_dict() }\n",
    "        torch.save(state, filename)\n",
    "    \n",
    "    def update_target_networks(self):\n",
    "        \"\"\"\n",
    "        A function to update the target networks\n",
    "        \"\"\"\n",
    "        weighSync(self.actor_target, self.actor)\n",
    "        weighSync(self.critic_target, self.critic)\n",
    "\n",
    "    def update_network(self):\n",
    "        \"\"\"\n",
    "        A function to update the function just once\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def getAverageReward(self):\n",
    "        ''' Run the policy and return average reward '''\n",
    "        rewards = []\n",
    "        s = self.test_env.reset()\n",
    "        done = False\n",
    "        while not(done):\n",
    "            a = self.actor(s).detach().squeeze().numpy()\n",
    "            s, r, done, _ = self.test_env.step(a)\n",
    "            rewards.append(r)\n",
    "        \n",
    "        avg_reward = sum(rewards)\n",
    "        assert isinstance(avg_reward, (int, float))\n",
    "        \n",
    "        return avg_reward\n",
    "\n",
    "    def train(self, max_num_steps):\n",
    "        \"\"\"\n",
    "        Train the policy for the given number of iterations\n",
    "        :param num_steps:The number of steps to train the policy for\n",
    "        \"\"\"\n",
    "        assert isinstance(max_num_steps, int) and max_num_steps>0\n",
    "        \n",
    "        gamma = self.gamma\n",
    "        s = self.env.reset()\n",
    "        done = False\n",
    "        for num_steps in range(max_num_steps):\n",
    "            # Reset env when it reached terminal state\n",
    "            if done:\n",
    "                self.num_episodes += 1\n",
    "                s = self.env.reset()\n",
    "            \n",
    "            a = self.actor.getAction(s, add_noise_flag = True, noise = 0.1)\n",
    "            s_prime, r, done, _ = self.env.step(a)\n",
    "            \n",
    "            # Storing transition in buffer\n",
    "            self.ReplayBuffer.buffer_add({\n",
    "                's': s,\n",
    "                'a': a,\n",
    "                'r': r,\n",
    "                's_prime': s_prime\n",
    "            })\n",
    "            # Sampling N points from buffer\n",
    "            minibatch = self.ReplayBuffer.buffer_sample(N = self.batch_size)\n",
    "            \n",
    "            loss_critic = torch.tensor([0]).float()\n",
    "            loss_actor = torch.tensor([0]).float()\n",
    "            \n",
    "            # Operating on minibatch\n",
    "            for sample in minibatch:\n",
    "                s, a, r, s_prime = sample['s'], sample['a'], sample['r'], sample['s_prime']\n",
    "                # Calculating critic loss\n",
    "                a_target = self.actor_target.getAction(s_prime, add_noise_flag=False)\n",
    "                y_i = r + gamma*self.critic_target(s_prime, a_target)\n",
    "                loss_critic = self.loss_fn(self.critic(s,a), y_i) #mse loss\n",
    "                \n",
    "                # Calculating actor loss\n",
    "                a = self.actor.getAction(s_prime, add_noise_flag=False)\n",
    "                loss_actor += self.critic(s, a)\n",
    "            \n",
    "            # zero gradients of optimizer\n",
    "            self.optimizer_critic.zero_grad()    \n",
    "            self.optimizer_actor.zero_grad()\n",
    "            # Update critic\n",
    "            loss_critic.backward()\n",
    "            self.optimizer_critic.step()\n",
    "            # Update actor\n",
    "            loss_actor /= -self.batch_size #multiplying with negative so it does gradient ascent\n",
    "            loss_actor.backward()\n",
    "            self.optimizer_actor.step()\n",
    "            # Update target networks\n",
    "            self.update_target_networks()\n",
    "            \n",
    "            if num_steps%100 == 0:\n",
    "                r = self.getAverageReward()\n",
    "                self.avg_rewards.append(r)\n",
    "                print('Num steps: {0} \\t Avg Reward: {1:.3f} \\t Obj(Actor): {2:.3f} \\t Loss(Critic): {3:.3f}'\\\n",
    "                      .format(num_steps, r, loss_actor.item(), loss_critic.item()))\n",
    "                \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_dir=/Users/adnanshahpurwala/anaconda3/envs/ece276c_venv/lib/python3.6/site-packages/pybullet_envs/bullet\n",
      "options= \n",
      "options= \n",
      "Num steps: 0 \t Avg Reward: -19.619 \t Obj(Actor): -0.073 \t Loss(Critic): 0.000\n",
      "Num steps: 50 \t Avg Reward: -17.554 \t Obj(Actor): 0.158 \t Loss(Critic): 0.133\n",
      "Num steps: 100 \t Avg Reward: -20.268 \t Obj(Actor): -0.145 \t Loss(Critic): 0.126\n",
      "Num steps: 150 \t Avg Reward: -19.361 \t Obj(Actor): 0.051 \t Loss(Critic): 0.004\n",
      "Num steps: 200 \t Avg Reward: -18.494 \t Obj(Actor): 0.037 \t Loss(Critic): 0.040\n",
      "Num steps: 250 \t Avg Reward: -20.747 \t Obj(Actor): 0.027 \t Loss(Critic): 0.002\n",
      "Num steps: 300 \t Avg Reward: -22.096 \t Obj(Actor): -0.024 \t Loss(Critic): 0.039\n",
      "Num steps: 350 \t Avg Reward: -21.973 \t Obj(Actor): 0.044 \t Loss(Critic): 0.125\n",
      "Num steps: 400 \t Avg Reward: -19.606 \t Obj(Actor): 0.009 \t Loss(Critic): 0.004\n",
      "Num steps: 450 \t Avg Reward: -18.405 \t Obj(Actor): -0.009 \t Loss(Critic): 0.020\n",
      "Num steps: 500 \t Avg Reward: -21.846 \t Obj(Actor): 0.026 \t Loss(Critic): 0.001\n",
      "Num steps: 550 \t Avg Reward: -17.365 \t Obj(Actor): 0.002 \t Loss(Critic): 0.009\n",
      "Num steps: 600 \t Avg Reward: -17.682 \t Obj(Actor): 0.028 \t Loss(Critic): 0.003\n",
      "Num steps: 650 \t Avg Reward: -20.468 \t Obj(Actor): -0.029 \t Loss(Critic): 0.112\n",
      "Num steps: 700 \t Avg Reward: -17.580 \t Obj(Actor): -0.003 \t Loss(Critic): 0.005\n",
      "Num steps: 750 \t Avg Reward: -0.247 \t Obj(Actor): 0.041 \t Loss(Critic): 0.019\n",
      "Num steps: 800 \t Avg Reward: -20.653 \t Obj(Actor): 0.014 \t Loss(Critic): 0.053\n",
      "Num steps: 850 \t Avg Reward: -17.167 \t Obj(Actor): 0.038 \t Loss(Critic): 0.008\n",
      "Num steps: 900 \t Avg Reward: -19.902 \t Obj(Actor): 0.044 \t Loss(Critic): 0.008\n",
      "Num steps: 950 \t Avg Reward: -22.540 \t Obj(Actor): 0.039 \t Loss(Critic): 0.000\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Define the environment\n",
    "    env = gym.make(\"modified_gym_env:ReacherPyBulletEnv-v1\", rand_init=True)\n",
    "    test_env = copy.deepcopy(env)\n",
    "    \n",
    "    # Define Deep Deterministic Policy Gradient object\n",
    "    ddpg_object = DDPG(\n",
    "        env,\n",
    "        test_env,\n",
    "        8,\n",
    "        2,\n",
    "        critic_lr=1e-3,\n",
    "        actor_lr=1e-3,\n",
    "        gamma=0.99,\n",
    "        batch_size=100,\n",
    "        ev_n_steps=100, #evaluate every n steps\n",
    "    )\n",
    "    # Train the policy\n",
    "    ddpg_object.train(10000)\n",
    "\n",
    "#     # Evaluate the final policy\n",
    "#     state = env.reset()\n",
    "#     done = False\n",
    "#     while not done:\n",
    "#         action = ddpg_object.actor(state).detach().squeeze().numpy()\n",
    "#         next_state, r, done, _ = env.step(action)\n",
    "#         env.render()\n",
    "#         time.sleep(0.1)\n",
    "#         state = next_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ddpg_object.save_actor(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ece276c_venv",
   "language": "python",
   "name": "ece276c_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
