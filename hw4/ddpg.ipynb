{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Learn a policy using DDPG for the reach task\"\"\"\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "import torch.nn as nn\n",
    "\n",
    "import gym\n",
    "import pybullet\n",
    "import pybulletgym.envs\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(1000)\n",
    "\n",
    "\n",
    "# TODO: A function to soft update target networks\n",
    "def weighSync(target_model, source_model, tau=0.001):\n",
    "    pass\n",
    "\n",
    "\n",
    "# TODO: Write the ReplayBuffer\n",
    "class Replay():\n",
    "    def __init__(self, buffer_size, init_length, state_dim, action_dim, env):\n",
    "        \"\"\"\n",
    "        A function to initialize the replay buffer.\n",
    "\n",
    "        param: init_length : Initial number of transitions to collect\n",
    "        param: state_dim : Size of the state space\n",
    "        param: action_dim : Size of the action space\n",
    "        param: env : gym environment object\n",
    "        \"\"\"\n",
    "        self.buffer_size = buffer_size\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        \n",
    "        self.buffer = deque() #list like object for which removing elements from left is faster\n",
    "        \n",
    "        s = env.reset()\n",
    "        for i in range(init_length):\n",
    "            a = env.action_space.sample()\n",
    "            s_prime, r, done, _ = env.step(a)\n",
    "            self.buffer.append({\n",
    "                's': s,\n",
    "                'a': a,\n",
    "                'r': r,\n",
    "                's_prime': s_prime\n",
    "            })\n",
    "\n",
    "    # TODO: Complete the function\n",
    "    def buffer_add(self, exp):\n",
    "        \"\"\"\n",
    "        A function to add a dictionary to the buffer\n",
    "        param: exp : A dictionary consisting of state, action, reward , next state and done flag\n",
    "        \"\"\"\n",
    "        assert isinstance(exp, dict) and len(exp) == 4\n",
    "        if len(self.buffer) < self.buffer_size:\n",
    "            self.buffer.append(exp)\n",
    "        else:\n",
    "            self.buffer.popleft() #removing the 1st element (left most element)\n",
    "            self.buffer.append(exp)\n",
    "\n",
    "    #TODO: Complete the function\n",
    "    def buffer_sample(self, N):\n",
    "        \"\"\"\n",
    "        A function to sample N points from the buffer\n",
    "        param: N : Number of samples to obtain from the buffer\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "# TODO: Define an Actor\n",
    "class Actor(nn.Module):\n",
    "    #TODO: Complete the function\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        \"\"\"\n",
    "        Initialize the network\n",
    "        param: state_dim : Size of the state space\n",
    "        param: action_dim: Size of the action space\n",
    "        \"\"\"\n",
    "        self.fc1 = nn.Linear(state_dim, 400)\n",
    "        self.hidden1 = nn.Linear(400, 300)\n",
    "        self.fc2 = nn.Linear(300, action_dim)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    #TODO: Complete the function\n",
    "    def forward(self, state):\n",
    "        \"\"\"\n",
    "        Define the forward pass\n",
    "        param: state: The state of the environment\n",
    "        \"\"\"\n",
    "        state = self.relu(self.fc1(state))\n",
    "        state = self.relu(self.hidden(state))\n",
    "        state = self.tanh(self.fc2(state))\n",
    "\n",
    "        return state\n",
    "\n",
    "\n",
    "# TODO: Define the Critic\n",
    "class Critic(nn.Module):\n",
    "    # TODO: Complete the function\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        \"\"\"\n",
    "        Initialize the critic\n",
    "        param: state_dim : Size of the state space\n",
    "        param: action_dim : Size of the action space\n",
    "        \"\"\"\n",
    "        self.fc1 = nn.Linear(state_dim + action_dim, 400)\n",
    "        self.hidden1 = nn.Linear(400, 300)\n",
    "        self.fc2 = nn.Linear(300, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    # TODO: Complete the function\n",
    "    def forward(self, state, action):\n",
    "        \"\"\" Define the forward pass of the critic \"\"\"\n",
    "        \n",
    "        x = torch.tensor([state, action]).double() #might need work to get it working\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.hidden(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "# TODO: Implement a DDPG class\n",
    "class DDPG():\n",
    "    def __init__(\n",
    "            self,\n",
    "            env,\n",
    "            action_dim,\n",
    "            state_dim,\n",
    "            critic_lr=3e-4,\n",
    "            actor_lr=3e-4,\n",
    "            gamma=0.99,\n",
    "            batch_size=100,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        param: env: An gym environment\n",
    "        param: action_dim: Size of action space\n",
    "        param: state_dim: Size of state space\n",
    "        param: critic_lr: Learning rate of the critic\n",
    "        param: actor_lr: Learning rate of the actor\n",
    "        param: gamma: The discount factor\n",
    "        param: batch_size: The batch size for training\n",
    "        \"\"\"\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.env = env\n",
    "\n",
    "        # TODO: Create a actor and actor_target\n",
    "        self.actor = Actor(state_dim, action_dim)\n",
    "        self.actor_target = Actor(state_dim, action_dim)\n",
    "        # TODO: Make sure that both networks have the same initial weights ###### TAKE CARE OF THIS\n",
    "\n",
    "        # TODO: Create a critic and critic_target object\n",
    "        self.critic = Critic(state_dim, action_dim)\n",
    "        self.critic_target = Critic(state_dim, action_dim)\n",
    "        # TODO: Make sure that both networks have the same initial weights ###### TAKE CARE OF THIS\n",
    "\n",
    "        #Define optimizer for actor and critic\n",
    "        self.optimizer_actor = optim.Adam(self.actor.parameters(), lr=actor_lr)\n",
    "        self.optimizer_critic = optim.Adam(self.critic.parameters(), lr=critic_lr)\n",
    "\n",
    "        # TODO: define a replay buffer\n",
    "        self.ReplayBuffer = None\n",
    "\n",
    "    # TODO: Complete the function\n",
    "    def update_target_networks(self):\n",
    "        \"\"\"\n",
    "        A function to update the target networks\n",
    "        \"\"\"\n",
    "        weighSync(self.actor_target, self.actor)\n",
    "        weighSync(self.critic_target, self.critic)\n",
    "\n",
    "    # TODO: Complete the function\n",
    "    def update_network(self):\n",
    "        \"\"\"\n",
    "        A function to update the function just once\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    # TODO: Complete the function\n",
    "    def train(self, num_steps):\n",
    "        \"\"\"\n",
    "        Train the policy for the given number of iterations\n",
    "        :param num_steps:The number of steps to train the policy for\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_dir=/Users/adnanshahpurwala/anaconda3/envs/ece276c_venv/lib/python3.6/site-packages/pybullet_envs/bullet\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Define the environment\n",
    "    env = gym.make(\"modified_gym_env:ReacherPyBulletEnv-v1\", rand_init=False)\n",
    "\n",
    "    # ddpg_object = DDPG(\n",
    "    #     env,\n",
    "    #     8,\n",
    "    #     2,\n",
    "    #     critic_lr=1e-3,\n",
    "    #     actor_lr=1e-3,\n",
    "    #     gamma=0.99,\n",
    "    #     batch_size=100,\n",
    "    # )\n",
    "    # # Train the policy\n",
    "    # ddpg_object.train(100)\n",
    "\n",
    "    # # Evaluate the final policy\n",
    "    # state = env.reset()\n",
    "    # done = False\n",
    "    # while not done:\n",
    "    #     action = ddpg_object.actor(state).detach().squeeze().numpy()\n",
    "    #     next_state, r, done, _ = env.step(action)\n",
    "    #     env.render()\n",
    "    #     time.sleep(0.1)\n",
    "    #     state = next_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.7988423, 0.6238568], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ece276c_venv",
   "language": "python",
   "name": "ece276c_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
