{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import copy\n",
    "import shutil\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "\n",
    "import gym\n",
    "import pybullet\n",
    "import pybulletgym.envs\n",
    "\n",
    "from collections import deque\n",
    "from operator import itemgetter\n",
    "from statistics import mean\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "parser = argparse.ArgumentParser(description='DDPG')\n",
    "parser.add_argument('-s', '--seed', type=int, help='seed for random initializations')\n",
    "parser.add_argument('-i', '--index', type=int, help='index of run')\n",
    "\n",
    "def weighSync(target_model, source_model, tau=0.001):\n",
    "    ''' A function to soft update target networks '''\n",
    "    assert isinstance(tau, float) and tau>0\n",
    "\n",
    "    for param_target, param_source in zip(target_model.parameters(), source_model.parameters()):\n",
    "        # Wrap in torch.no_grad() because weights have requires_grad=True, \n",
    "        # but we don't need to track this in autograd\n",
    "        with torch.no_grad():\n",
    "            param_target = tau*param_source + (1-tau)*param_target\n",
    "    \n",
    "    return target_model, source_model\n",
    "\n",
    "\n",
    "class Replay():\n",
    "    def __init__(self, buffer_size, init_length, state_dim, action_dim, env):\n",
    "        \"\"\"\n",
    "        A function to initialize the replay buffer.\n",
    "\n",
    "        param: init_length : Initial number of transitions to collect\n",
    "        param: state_dim : Size of the state space\n",
    "        param: action_dim : Size of the action space\n",
    "        param: env : gym environment object\n",
    "        \"\"\"\n",
    "        assert isinstance(buffer_size, int) and buffer_size>0\n",
    "        assert isinstance(init_length, int) and init_length>0\n",
    "        assert isinstance(state_dim, int) and state_dim>0\n",
    "        assert isinstance(action_dim, int) and action_dim>0\n",
    "        \n",
    "        self.buffer_size = buffer_size\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        \n",
    "        self.buffer = deque() #list like object for which removing elements from left is faster\n",
    "        \n",
    "        s = env.reset()\n",
    "        for i in range(init_length):\n",
    "            a = env.action_space.sample()\n",
    "            s_prime, r, done, _ = env.step(a)\n",
    "            self.buffer.append((s,a,r,s_prime))\n",
    "    \n",
    "    def __len__(self):\n",
    "        ''' Return number of elements in buffer'''\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def buffer_add(self, exp):\n",
    "        \"\"\"\n",
    "        A function to add a dictionary to the buffer\n",
    "        param: exp : A tuple consisting of (state, action, reward, next state) in that order\n",
    "        \"\"\"\n",
    "        assert isinstance(exp, tuple) and len(exp) == 4\n",
    "        assert len(self.buffer) <= self.buffer_size, 'Buffer size exceeded. You fucked up'\n",
    "        \n",
    "        if len(self) < self.buffer_size:\n",
    "            self.buffer.append(exp)\n",
    "        else:\n",
    "            self.buffer.popleft() #removing the 1st element (left most element)\n",
    "            self.buffer.append(exp)\n",
    "\n",
    "    def buffer_sample(self, N):\n",
    "        \"\"\"\n",
    "        A function to sample N points from the buffer\n",
    "        param: N : Number of samples to obtain from the buffer\n",
    "        \"\"\"\n",
    "        assert isinstance(N, int) and N>0\n",
    "        indices = list(np.random.randint(low=0, high=len(self), size=N, dtype='int'))\n",
    "        sample = itemgetter(*indices)(self.buffer) #extarct values at indices from buffer\n",
    "        \n",
    "        assert len(sample) == N, 'You fucked up sampling bruh'\n",
    "        return sample\n",
    "\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        \"\"\"\n",
    "        Initialize the network\n",
    "        param: state_dim : Size of the state space\n",
    "        param: action_dim: Size of the action space\n",
    "        \"\"\"\n",
    "        assert isinstance(state_dim, int) and state_dim>0\n",
    "        assert isinstance(action_dim, int) and action_dim>0\n",
    "        \n",
    "        super(Actor, self).__init__()\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        \n",
    "        # NN layers and activations\n",
    "        self.fc1 = nn.Linear(state_dim, 400)\n",
    "        self.hidden1 = nn.Linear(400, 300)\n",
    "        self.fc2 = nn.Linear(300, action_dim)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"\n",
    "        Define the forward pass\n",
    "        param: state: The state of the environment\n",
    "        \"\"\"\n",
    "        if not(isinstance(state, torch.Tensor)):\n",
    "            state = torch.from_numpy(state).float()\n",
    "        \n",
    "        x = state\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.hidden1(x))\n",
    "        x = self.tanh(self.fc2(x))\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def getAction(self, state, add_noise_flag = False, noise = 0.1):\n",
    "        '''\n",
    "        Returns an action by doing a forward pass. If add_noise_flag is True, \n",
    "        action is sampled from a multivariate Normal distributio with stddev = noise and mean = output of net\n",
    "        \n",
    "        :rtype: np.ndarray\n",
    "        '''\n",
    "        assert isinstance(state, np.ndarray)\n",
    "        assert isinstance(noise, (int, float)) and noise >=0\n",
    "        assert isinstance(add_noise_flag, bool)\n",
    "        \n",
    "        state = torch.from_numpy(state).float()\n",
    "        action = self.forward(state) #forward pass\n",
    "        \n",
    "        if add_noise_flag:\n",
    "            # Sampling from the nD Gaussian\n",
    "            m = MultivariateNormal(action, torch.eye(self.action_dim)*noise)\n",
    "            action = m.sample()\n",
    "        \n",
    "        return action.detach().squeeze().numpy()\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        \"\"\"\n",
    "        Initialize the critic\n",
    "        param: state_dim : Size of the state space\n",
    "        param: action_dim : Size of the action space\n",
    "        \"\"\"\n",
    "        assert isinstance(state_dim, int) and state_dim>0\n",
    "        assert isinstance(action_dim, int) and action_dim>0\n",
    "        \n",
    "        super(Critic, self).__init__()\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        \n",
    "        # NN layers and activations\n",
    "        self.fc1 = nn.Linear(state_dim + action_dim, 400)\n",
    "        self.hidden1 = nn.Linear(400, 300)\n",
    "        self.fc2 = nn.Linear(300, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        \"\"\" Define the forward pass of the critic \"\"\"\n",
    "        assert isinstance(state, np.ndarray)\n",
    "        assert isinstance(action, np.ndarray)\n",
    "        assert state.shape[1] == self.state_dim, 'state must be of dim (batch_size, %d)'%self.state_dim\n",
    "        assert action.shape[1] == self.action_dim, 'action must be of dim (batch_size, %d)'%self.action_dim\n",
    "        \n",
    "        state, action = torch.from_numpy(state).float(), torch.from_numpy(action).float() #numpy to torch tensor\n",
    "        try:\n",
    "            x = torch.cat((state, action), dim=0) #concatenating to form input\n",
    "        except RuntimeError:\n",
    "            x = torch.cat((state, action), dim=1) #concatenating to form input\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.hidden1(x))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class DDPG():\n",
    "    def __init__(\n",
    "            self,\n",
    "            env,\n",
    "            test_env,\n",
    "            state_dim,    \n",
    "            action_dim,\n",
    "            critic_lr=3e-4,\n",
    "            actor_lr=3e-4,\n",
    "            gamma=0.99,\n",
    "            batch_size=100,\n",
    "            ev_n_steps=100,\n",
    "            verbose=False\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Implementing DPPG algorithm from paper - Continuous control with deep reinforcement learning\n",
    "        link - https://arxiv.org/pdf/1509.02971.pdf\n",
    "        \n",
    "        param: env: An gym environment\n",
    "        param: action_dim: Size of action space\n",
    "        param: state_dim: Size of state space\n",
    "        param: critic_lr: Learning rate of the critic\n",
    "        param: actor_lr: Learning rate of the actor\n",
    "        param: gamma: The discount factor\n",
    "        param: batch_size: The batch size for training\n",
    "        \"\"\"\n",
    "\n",
    "        assert isinstance(state_dim, int) and state_dim>0\n",
    "        assert isinstance(action_dim, int) and action_dim>0\n",
    "        assert isinstance(batch_size, int) and batch_size>0\n",
    "        assert isinstance(critic_lr, (int, float)) and critic_lr>0\n",
    "        assert isinstance(actor_lr, (int, float)) and actor_lr>0\n",
    "        assert isinstance(gamma, (int, float)) and gamma>0\n",
    "        assert isinstance(ev_n_steps, (int, float)) and ev_n_steps>0\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.ev_n_steps = ev_n_steps\n",
    "        self.env = env\n",
    "        self.test_env = test_env\n",
    "        self.num_episodes = 0\n",
    "        self.avg_rewards = []\n",
    "        self.obj_actor = []\n",
    "        self.loss_critic = []\n",
    "\n",
    "        # Create a actor and actor_target with same initial weights\n",
    "        self.actor = Actor(state_dim, action_dim)\n",
    "        self.actor = self.init_weights(self.actor) #initialize weights according to paper\n",
    "        self.actor_target = copy.deepcopy(self.actor) #both networks have the same initial weights \n",
    "\n",
    "        # Create a critic and critic_target with same initial weights\n",
    "        self.critic = Critic(state_dim, action_dim)\n",
    "        self.critic = self.init_weights(self.critic) #initialize weights according to paper\n",
    "        self.critic_target = copy.deepcopy(self.critic) #both networks have the same initial weights \n",
    "\n",
    "        # Define optimizer for actor and critic\n",
    "        self.optimizer_actor = optim.Adam(self.actor.parameters(), lr=actor_lr)\n",
    "        self.optimizer_critic = optim.Adam(self.critic.parameters(), lr=critic_lr)\n",
    "        self.loss_fn = nn.MSELoss(reduction='mean')\n",
    "\n",
    "        # Define a replay buffer\n",
    "        self.ReplayBuffer = Replay(10000, 1000, state_dim, action_dim, self.env)\n",
    "    \n",
    "    def init_weights(self, network):\n",
    "        '''\n",
    "        Initialize weights (as mentioned in paper) from a uniform distribution \n",
    "        based on the fan-in of the layer\n",
    "        \n",
    "        WARNING: Will only work if each layer is fully connected\n",
    "        '''\n",
    "        \n",
    "        for name, param in network.named_parameters():\n",
    "            if 'bias' in name: #if bias param, use f of the same layer\n",
    "                f = last_f\n",
    "            else:\n",
    "                f = param.shape[1] #picking 2nd dim = number of inputs for that layer\n",
    "                last_f = f\n",
    "            # Initialize weights by sampling from uniform dist.\n",
    "            assert isinstance(f, int) and f>0, 'fan in must be int and greater than 0'\n",
    "            nn.init.uniform_(param.data, a = -1/np.sqrt(f), b = 1/np.sqrt(f))\n",
    "        \n",
    "        return network\n",
    "    \n",
    "    def save_actor(self, index):\n",
    "        ''' Saves the policy NN'''\n",
    "        filename = 'q1_policy_' + str(index) + '.pth.tar'\n",
    "        state = { 'state_dict': self.actor.state_dict(),\n",
    "                 'optimizer': self.optimizer_actor.state_dict() }\n",
    "        torch.save(state, filename)\n",
    "        torch.save(self.best_actor_state, 'best'+filename)\n",
    "    \n",
    "    def update_target_networks(self):\n",
    "        \"\"\"\n",
    "        A function to update the target networks\n",
    "        \"\"\"\n",
    "        weighSync(self.actor_target, self.actor)\n",
    "        weighSync(self.critic_target, self.critic)\n",
    "    \n",
    "    def getAverageReward(self):\n",
    "        ''' Run the policy and return average reward '''\n",
    "        rewards = []\n",
    "        s = self.test_env.reset()\n",
    "        done = False\n",
    "        while not(done):\n",
    "            a = self.actor(s).detach().squeeze().numpy()\n",
    "            s, r, done, _ = self.test_env.step(a)\n",
    "            rewards.append(r)\n",
    "        \n",
    "        avg_reward = sum(rewards)\n",
    "        assert isinstance(avg_reward, (int, float))\n",
    "        \n",
    "        return avg_reward\n",
    "\n",
    "    def train(self, max_num_steps):\n",
    "        \"\"\"\n",
    "        Train the policy for the given number of iterations\n",
    "        :param num_steps:The number of steps to train the policy for\n",
    "        \"\"\"\n",
    "        assert isinstance(max_num_steps, int) and max_num_steps>0\n",
    "        \n",
    "        gamma = self.gamma\n",
    "        state = self.env.reset()\n",
    "        done = False\n",
    "        best_r = -np.inf\n",
    "        # Training starts now\n",
    "        for num_steps in tqdm(range(max_num_steps)):\n",
    "            # Reset env when it reached terminal state\n",
    "            if done:\n",
    "                self.num_episodes += 1\n",
    "                state = self.env.reset()\n",
    "            \n",
    "            action = self.actor.getAction(state, add_noise_flag = True, noise = 0.1)\n",
    "            state_next, rewd, done, _ = self.env.step(action)\n",
    "            \n",
    "            # Storing transition in buffer\n",
    "            self.ReplayBuffer.buffer_add((state,action,rewd,state_next))\n",
    "            state = state_next\n",
    "\n",
    "            # Sampling N points from buffer\n",
    "            minibatch = self.ReplayBuffer.buffer_sample(N = self.batch_size)\n",
    "\n",
    "            \n",
    "            # Operating on minibatch\n",
    "            s = np.array([el[0] for el in minibatch])                        #dim (batch_size, state_dim)\n",
    "            a = np.array([el[1] for el in minibatch])                        #dim (batch_size, action_dim)\n",
    "            r = torch.Tensor([el[2] for el in minibatch]).unsqueeze(dim=1)   #dim (batch_size, 1)\n",
    "            s_prime = np.array([el[3] for el in minibatch])                  #dim (batch_size, state_dim)\n",
    "            \n",
    "            a_target = self.actor_target.getAction(s_prime, add_noise_flag=False) #dim(batch_size, action_dim)\n",
    "            y_i = r + gamma*self.critic_target(s_prime, a_target)            #dim (batch_size, 1)\n",
    "            loss_critic = self.loss_fn(self.critic(s,a), y_i) #mse loss      #dim (batch_size, 1)\n",
    "            \n",
    "            a = self.actor.getAction(s_prime, add_noise_flag=False)\n",
    "            obj_actor = self.critic(s, a).mean()\n",
    "            \n",
    "            # Zero gradients of optimizer\n",
    "            self.optimizer_critic.zero_grad()    \n",
    "            self.optimizer_actor.zero_grad()\n",
    "            # Update critic\n",
    "            loss_critic.backward()\n",
    "            self.optimizer_critic.step()\n",
    "            # Update actor\n",
    "            obj_actor *= -1 #multiplying with negative so it does gradient ascent\n",
    "            obj_actor.backward()\n",
    "            self.optimizer_actor.step()\n",
    "            # Update target networks\n",
    "            self.update_target_networks()\n",
    "            \n",
    "            # Store losses\n",
    "            self.obj_actor.append(obj_actor.item())\n",
    "            self.loss_critic.append(loss_critic.item())\n",
    "            \n",
    "            if num_steps%self.ev_n_steps == 0:\n",
    "                r = self.getAverageReward()\n",
    "                # Saving best actor model till now\n",
    "                is_best = r > best_r\n",
    "                best_r = max(r, best_r)\n",
    "                if is_best:\n",
    "                    self.best_actor_state = {'state_dict': self.actor.state_dict(),\n",
    "                                             'optimizer': self.optimizer_actor.state_dict(),\n",
    "                                             'avg_reward': r,\n",
    "                                             'obj_actor': obj_actor.item(),\n",
    "                                             'loss_critic': loss_critic.item()\n",
    "                                            }\n",
    "                \n",
    "                self.avg_rewards.append(r)\n",
    "                if verbose:\n",
    "                    print('Num steps: {0} \\t Avg Reward: {1:.3f} \\t Obj(Actor): {2:.3f} \\t Loss(Critic): {3:.3f}'\n",
    "                          .format(num_steps, r, obj_actor.item(), loss_critic.item()))\n",
    "                \n",
    "                \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "options= \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 7/5000 [00:00<07:45, 10.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "options= \n",
      "Num steps: 0 \t Avg Reward: -19.398 \t Obj(Actor): 0.076 \t Loss(Critic): 1.834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1008/5000 [00:20<01:32, 43.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num steps: 1000 \t Avg Reward: -19.842 \t Obj(Actor): -0.303 \t Loss(Critic): 0.468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2010/5000 [00:42<01:10, 42.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num steps: 2000 \t Avg Reward: -22.384 \t Obj(Actor): -0.183 \t Loss(Critic): 0.382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3010/5000 [01:02<00:45, 43.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num steps: 3000 \t Avg Reward: -22.918 \t Obj(Actor): -0.056 \t Loss(Critic): 0.233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4006/5000 [01:23<00:21, 45.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num steps: 4000 \t Avg Reward: -3.541 \t Obj(Actor): -0.015 \t Loss(Critic): 0.225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [01:44<00:00, 47.87it/s]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # Seed value\n",
    "    torch.manual_seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    \n",
    "    # Define the environment\n",
    "    env = gym.make(\"modified_gym_env:ReacherPyBulletEnv-v1\", rand_init=True)\n",
    "    test_env = copy.deepcopy(env)\n",
    "    plot = True\n",
    "    verbose = True\n",
    "    \n",
    "    # Define Deep Deterministic Policy Gradient object\n",
    "    ddpg_object = DDPG(\n",
    "        env,\n",
    "        test_env,\n",
    "        8,\n",
    "        2,\n",
    "        critic_lr=1e-3,\n",
    "        actor_lr=1e-4,\n",
    "        gamma=0.99,\n",
    "        batch_size=200,\n",
    "        ev_n_steps=1000, #evaluate every n steps\n",
    "        verbose=verbose\n",
    "    )\n",
    "    # Train the policy\n",
    "    ddpg_object.train(int(3e3))\n",
    "    \n",
    "    # Save actor\n",
    "    ddpg_object.save_actor(args.index)\n",
    "    \n",
    "    if plot:\n",
    "        fig, axs = plt.subplots(3,1, figsize=(10,15))\n",
    "        axs = axs.flatten()\n",
    "        axs[0].plot(ddpg_object.avg_rewards)\n",
    "        axs[0].set_ylabel('Avg Rewards')\n",
    "        axs[0].set_title(' Avg Rewards vs iterations ')\n",
    "    #     axs[1].plot(ddpg_object.obj_actor[::ddpg_object.ev_n_steps])\n",
    "        axs[1].plot(ddpg_object.obj_actor)\n",
    "        axs[1].set_ylabel('Actor Obj')\n",
    "        axs[1].set_title(' Actor Obj vs iterations ')\n",
    "    #     axs[2].plot(ddpg_object.loss_critic[::ddpg_object.ev_n_steps])\n",
    "        axs[2].plot(ddpg_object.loss_critic)\n",
    "        axs[2].set_ylabel('Critic Loss')\n",
    "        axs[2].set_title(' Critic Loss vs iterations ')\n",
    "        plt.savefig('index_'+str(args.index)+'.png', dpi=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ece276c_venv",
   "language": "python",
   "name": "ece276c_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
