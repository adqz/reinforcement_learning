{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_arguments(fn): # note that function as input\n",
    "    def new_function(*args,**kwargs): # we've seen these arguments before\n",
    "        print ('positional arguments:')\n",
    "        print (args)\n",
    "        print ('keyword arguments:')\n",
    "        print (kwargs)\n",
    "        return fn(*args,**kwargs) # return a function\n",
    "    return new_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrozenLake:\n",
    "    '''\n",
    "    A class written to solve Question 1 in HW2, ECE276C at UCSD\n",
    "    For more on frozenlake, check out - https://github.com/openai/gym/blob/master/gym/envs/toy_text/frozen_lake.py\n",
    "    \n",
    "    The action have the following meanings:\n",
    "        LEFT = 0\n",
    "        DOWN = 1\n",
    "        RIGHT = 2\n",
    "        UP = 3\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Initialize the frozen-lake environment\n",
    "        '''\n",
    "        self.env = gym.make (\"FrozenLake-v0\")\n",
    "        self.numStates = self.env.observation_space.n\n",
    "        self.numActions = self.env.action_space.n\n",
    "        self.dt = 1\n",
    "        \n",
    "        self.initial_state = self.env.reset() #reset env and return initial state\n",
    "        print('Environment map:')\n",
    "        self.printMap()\n",
    "\n",
    "    def printMap(self):\n",
    "        print(self.env.desc)\n",
    "    \n",
    "    def generateRollout(self, policy=None, maxT = 100, initial_state = 0):\n",
    "        '''\n",
    "        A rollout is a series of (state, action) pairs which goes on until time maxT or we reach a terminal state.\n",
    "        A terminal state is reached when done = True in the following statement:\n",
    "        \n",
    "        >>> obs,r,done = env.step(action)\n",
    "        '''\n",
    "        assert isinstance(maxT, (int, float)), 'maxT needs to be int or float'\n",
    "        assert isinstance(initial_state, int)\n",
    "        assert initial_state in range(self.numStates)\n",
    "        \n",
    "        states = [initial_state]\n",
    "        actions = []\n",
    "        \n",
    "        t = 0\n",
    "        while(t < maxT):\n",
    "            if policy == None:\n",
    "                a = self.env.action_space.sample()\n",
    "            else:\n",
    "                a = policy(self, states[-1])\n",
    "            # Take a step using action a\n",
    "            next_state, r, done, info = env.step(a)\n",
    "            # Save next state and the action that led to it\n",
    "            states.append(next_state)\n",
    "            actions.append(a)\n",
    "            if done:\n",
    "                break\n",
    "            t = t + self.dt\n",
    "\n",
    "        assert len(states) - len(actions) == 1, 'Number of actions should be 1 less than number of states'\n",
    "        rollout = {\n",
    "            'state': states,\n",
    "            'action': actions,\n",
    "        }\n",
    "        \n",
    "        return rollout\n",
    "    \n",
    "#     @log_arguments\n",
    "    def TestPolicy(self, policy, num_trials=100, timeLimit = False, maxT=5000):\n",
    "        '''\n",
    "        Returns the average rate of successful episodes over 100 trials for a deterministic policy\n",
    "        '''\n",
    "        assert isinstance(policy, np.ndarray) and len(policy) == self.numStates\n",
    "        assert isinstance(num_trials, int) and num_trials>0\n",
    "        assert isinstance(timeLimit, bool)\n",
    "        assert isinstance(maxT, int) and maxT>0\n",
    "\n",
    "        \n",
    "        success_count = 0\n",
    "        for i in range(num_trials):\n",
    "            t = 0\n",
    "            state = int(self.env.reset()) #resetting state to initial position\n",
    "            while(True):\n",
    "                a = policy[state] #getting action from policy\n",
    "                next_state, r, done, info = self.env.step(a) #taking a step using action\n",
    "#                 print('next_state, r = ', next_state, r)\n",
    "                # Check if we reached goal, i.e. check for success\n",
    "                if (done and r == 1.0):\n",
    "                    success_count += 1\n",
    "                    break\n",
    "                \n",
    "                # Checking if we fell in a hole\n",
    "                elif done:\n",
    "                    break\n",
    "                \n",
    "                if timeLimit and t>maxT:\n",
    "                    print('Max time exceeded. Breaking out of loop')\n",
    "                    break\n",
    "                state = next_state\n",
    "                t += self.dt\n",
    "        return success_count/num_trials\n",
    "\n",
    "    def LearnModel(self, num_samples = 100000):\n",
    "        '''\n",
    "        Returns transition probabilities and reward function\n",
    "        \n",
    "        p(s'|a, s) is accessed by typing p[s][a][s']\n",
    "        r(s,a,s') is accessed by typing r[s][a][s']\n",
    "        '''\n",
    "        assert isinstance(num_samples, int) and num_samples > 0\n",
    "        \n",
    "        self.env.reset()\n",
    "        p = np.zeros((self.numStates, self.numActions, self.numStates))\n",
    "        r = np.zeros((self.numStates, self.numActions, self.numStates))\n",
    "        counter = np.zeros((self.numStates, self.numActions))\n",
    "        self.count_s = np.zeros(self.numStates)\n",
    "        self.count_a = np.zeros(self.numActions)\n",
    "        \n",
    "        for i in range(num_samples):\n",
    "            s = np.random.randint(low = 0, high = self.numStates, dtype = int)\n",
    "            a = np.random.randint(low = 0, high = self.numActions, dtype = int)\n",
    "            self.count_s[s] += 1; self.count_a[a] += 1\n",
    "            \n",
    "            self.env.unwrapped.s = s #setting current state to randomly chosen state\n",
    "            s_prime, reward, _, _ = self.env.step(a)\n",
    "            \n",
    "            p[s][a][s_prime] += 1\n",
    "            r[s][a][s_prime] += reward\n",
    "            counter[s][a] += 1\n",
    "        \n",
    "        #use itertools instead\n",
    "        for s in range(self.numStates):\n",
    "            for a in range(self.numActions):\n",
    "                assert counter[s][a] != 0, 'Zero occurences of state-action pair. Cannot divide by 0'\n",
    "                p[s][a][:] = p[s][a][:]/counter[s][a]\n",
    "                r[s][a][:] = r[s][a][:]/counter[s][a]\n",
    "        \n",
    "        # Checking that probabilities sum to 1        \n",
    "        for s in range(self.numStates):\n",
    "            for a in range(self.numActions):\n",
    "                assert abs(sum(p[s,a,:])-1.0) < 1e-4, 'Probabilities dont sum to 1 --> %f' % sum(p[s,a,:])\n",
    "        \n",
    "        return p, r, counter\n",
    "    \n",
    "    def initializeValueAndPolicyFunction(self, initial_value = 0):\n",
    "        '''\n",
    "        Initializes Value and Policy function and returns them\n",
    "        All initial values for Value function are set to param initial_value\n",
    "        Initial values for Policy function are chosen randomly between [0, number of actions)\n",
    "        '''\n",
    "        assert isinstance(initial_value, (int, float))\n",
    "        \n",
    "        V = np.zeros((self.numStates))\n",
    "        policy = np.zeros((self.numStates))\n",
    "        \n",
    "        for s in range(self.numStates):\n",
    "            a = np.random.randint(low = 0, high = self.numActions, dtype = int)\n",
    "            V[s] = initial_value\n",
    "            policy[s] = a\n",
    "        \n",
    "        return V, policy\n",
    "    \n",
    "    def evaluatePolicy(self, V, policy, gamma = 0.9):\n",
    "        '''\n",
    "        Evaluates policy for 1 iteration\n",
    "        '''\n",
    "        assert isinstance(V, np.ndarray) and len(V) == self.numStates\n",
    "        assert isinstance(gamma, float) and 0.0<gamma<1.0\n",
    "        \n",
    "        V_new = np.zeros_like(V)\n",
    "        for s in range(self.numStates):\n",
    "            a = policy[s]\n",
    "            pf_s = self.env.P[s][a] #prob distribution over states for taking action a at state s\n",
    "            # Calculating expected value of Value function coz of policy\n",
    "            exp_V = 0.0\n",
    "            for possible_sa_pairs in pf_s:\n",
    "                prob, next_state, r, _ = possible_sa_pairs\n",
    "                exp_V += prob*( r + gamma*V[next_state] ) \n",
    "            V_new[s] = exp_V\n",
    "        return V_new\n",
    "                \n",
    "    def checkPolicyConvergence(self, V_old, V_new, threshold = 1e-3):\n",
    "        '''\n",
    "        Checks for convergence by checking if the maximum difference between value of all states \n",
    "        is less than some threshold\n",
    "        '''\n",
    "        assert isinstance(threshold, float) and 0 < threshold < 1\n",
    "        assert isinstance(V_old, np.ndarray) and len(V_old) == self.numStates\n",
    "        assert isinstance(V_new, np.ndarray) and len(V_new) == self.numStates\n",
    "        \n",
    "        max_diff = np.max(np.abs(V_old - V_new))\n",
    "        success = max_diff<threshold\n",
    "        return success, max_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def policyOne(fl, s):\n",
    "#     '''\n",
    "#     Returns action a by applying for the policy pi(s) = (s+1)%4 for state s\n",
    "#     '''\n",
    "#     assert isinstance(s, int)\n",
    "#     assert s in range(fl.numStates)\n",
    "    \n",
    "#     a = (s+1)%4 #policy pi(s)\n",
    "#     assert a in range(fl.numActions)\n",
    "#     return a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment map:\n",
      "[[b'S' b'F' b'F' b'F']\n",
      " [b'F' b'H' b'F' b'H']\n",
      " [b'F' b'F' b'F' b'H']\n",
      " [b'H' b'F' b'F' b'G']]\n",
      "\n",
      "Success Rate for policy is: 0.000\n"
     ]
    }
   ],
   "source": [
    "fl = FrozenLake()\n",
    "# Forming test policy\n",
    "policyTest = np.zeros((fl.numStates))\n",
    "for s in range(fl.numStates):\n",
    "    policyTest[s] = (s+1)%4\n",
    "\n",
    "success_rate = fl.TestPolicy(policyTest, num_trials = 100, maxT = 5000)\n",
    "print('\\nSuccess Rate for policy is: %.3f' % (success_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "p, r, counter = fl.LearnModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Evaluation converged in 9 iterations\n",
      "avg_success_rate =  0.33\n",
      "Policy Evaluation converged in 17 iterations\n",
      "avg_success_rate =  0.38\n",
      "Policy Evaluation converged in 8 iterations\n",
      "avg_success_rate =  0.68\n",
      "Policy Evaluation converged in 3 iterations\n",
      "avg_success_rate =  0.71\n",
      "----->Policy improvement converged in 3 iterations\n"
     ]
    }
   ],
   "source": [
    "# 1. Initialize value and policy function\n",
    "V, policy = fl.initializeValueAndPolicyFunction()\n",
    "\n",
    "counter2 = 0\n",
    "while True:\n",
    "    # 2. Repeated policy Evaluation\n",
    "    counter1 = 0\n",
    "    while (True):\n",
    "        # Step 1: Policy evaluation\n",
    "        V_new = fl.evaluatePolicy(V, policy=policy, gamma = 0.9)\n",
    "        success, max_diff = fl.checkPolicyConvergence(V, V_new, threshold = 1e-3)\n",
    "        V = V_new\n",
    "        if success:\n",
    "#             print('Policy eval converged. max diff = ', max_diff)\n",
    "            break\n",
    "        else:\n",
    "            counter1 += 1\n",
    "    print('Policy Evaluation converged in %d iterations' % counter1)\n",
    "    \n",
    "    # 3. Policy improvement\n",
    "    policy_stable = True\n",
    "    gamma = 0.9\n",
    "\n",
    "    for s in range(fl.numStates):\n",
    "        old_action = policy[s]\n",
    "        # Calculating q_pi(s,a) (from slides) for all actions over which we'll do argmax to find optimal action a\n",
    "        q_pi_sa = np.zeros((fl.numActions))\n",
    "        for a in range(fl.numActions):\n",
    "            pf_s = fl.env.P[s][a] #prob distribution over states for taking action a at state s\n",
    "            expectation = 0.0\n",
    "            for possible_sa_pairs in pf_s:\n",
    "                prob, next_state, r, _ = possible_sa_pairs\n",
    "                expectation += prob*( r + gamma*V[next_state] ) \n",
    "            q_pi_sa[a] = expectation\n",
    "        # Updating policy\n",
    "        optimal_a = np.argmax(q_pi_sa)\n",
    "        policy[s] = optimal_a \n",
    "\n",
    "        if old_action != optimal_a:\n",
    "            policy_stable = False\n",
    "    \n",
    "    avg_success_rate = fl.TestPolicy(policy, num_trials = 100, maxT = 5000)\n",
    "    print('avg_success_rate = ', avg_success_rate)\n",
    "    if policy_stable:\n",
    "        break;\n",
    "    counter2 += 1\n",
    "print('----->Policy improvement converged in %d iterations' % counter2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.06302092 0.05648566 0.07094107 0.05232865]\n",
      " [0.086699   0.         0.11047763 0.        ]\n",
      " [0.14151271 0.24507633 0.29794938 0.        ]\n",
      " [0.         0.37831504 0.63821312 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(V.reshape(4,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 3. 0. 3.]\n",
      " [0. 0. 0. 0.]\n",
      " [3. 1. 0. 0.]\n",
      " [0. 2. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(policy.reshape(4,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.73"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fl.TestPolicy(policy, num_trials = 100, maxT = 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "## Plotting transition probabilites for a certain action a\n",
    "# fig, axs = plt.subplots(figsize = (6,6))\n",
    "# axs = plt.imshow(p[:,0,:], cmap='gray')\n",
    "# plt.colorbar(axs,fraction=0.046, pad=0.04)\n",
    "## Checking if states and actions were sampled uniformly\n",
    "# fig, axs = plt.subplots(1,2, figsize = (15,5))\n",
    "# axs[0].bar(range(fl.numStates), fl.count_s)\n",
    "# axs[1].bar(range(fl.numActions), fl.count_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir(gym.envs.toy_text.discrete.DiscreteEnv)\n",
    "# import inspect\n",
    "# inspect.getmembers(gym.envs.toy_text.discrete.DiscreteEnv, lambda a: not(inspect.isroutine(a)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ece276c_venv",
   "language": "python",
   "name": "ece276c_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
