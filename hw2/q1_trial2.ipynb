{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from collections import defaultdict\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_arguments(fn): # note that function as input\n",
    "    def new_function(*args,**kwargs): # we've seen these arguments before\n",
    "        print ('positional arguments:')\n",
    "        print (args)\n",
    "        print ('keyword arguments:')\n",
    "        print (kwargs)\n",
    "        return fn(*args,**kwargs) # return a function\n",
    "    return new_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrozenLake:\n",
    "    '''\n",
    "    A class written to solve Question 1 in HW2, ECE276C at UCSD\n",
    "    For more on frozenlake, check out - https://github.com/openai/gym/blob/master/gym/envs/toy_text/frozen_lake.py\n",
    "    \n",
    "    The action have the following meanings:\n",
    "        LEFT = 0\n",
    "        DOWN = 1\n",
    "        RIGHT = 2\n",
    "        UP = 3\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Initialize the frozen-lake environment\n",
    "        '''\n",
    "        self.env = gym.make (\"FrozenLake-v0\")\n",
    "        self.numStates = self.env.observation_space.n\n",
    "        self.numActions = self.env.action_space.n\n",
    "        self.dt = 1\n",
    "        \n",
    "        self.initial_state = self.env.reset() #reset env and return initial state\n",
    "        print('Environment map:')\n",
    "        self.printMap()\n",
    "\n",
    "    def printMap(self):\n",
    "        print(self.env.desc)\n",
    "    \n",
    "    def generateRollout(self, policy=None, maxT = 100, initial_state = 0):\n",
    "        '''\n",
    "        A rollout is a series of (state, action) pairs which goes on until time maxT or we reach a terminal state.\n",
    "        A terminal state is reached when done = True in the following statement:\n",
    "        \n",
    "        >>> obs,r,done = env.step(action)\n",
    "        '''\n",
    "        assert isinstance(maxT, (int, float)), 'maxT needs to be int or float'\n",
    "        assert isinstance(initial_state, int)\n",
    "        assert initial_state in range(self.numStates)\n",
    "        \n",
    "        states = [initial_state]\n",
    "        actions = []\n",
    "        \n",
    "        t = 0\n",
    "        while(t < maxT):\n",
    "            if policy == None:\n",
    "                a = self.env.action_space.sample()\n",
    "            else:\n",
    "                a = policy(self, states[-1])\n",
    "            # Take a step using action a\n",
    "            next_state, r, done, info = env.step(a)\n",
    "            # Save next state and the action that led to it\n",
    "            states.append(next_state)\n",
    "            actions.append(a)\n",
    "            if done:\n",
    "                break\n",
    "            t = t + self.dt\n",
    "\n",
    "        assert len(states) - len(actions) == 1, 'Number of actions should be 1 less than number of states'\n",
    "        rollout = {\n",
    "            'state': states,\n",
    "            'action': actions,\n",
    "        }\n",
    "        \n",
    "        return rollout\n",
    "    \n",
    "#     @log_arguments\n",
    "    def TestPolicy(self, policy, num_trials=100, timeLimit = False, maxT=5000):\n",
    "        '''\n",
    "        Returns the average rate of successful episodes over 100 trials for a deterministic policy\n",
    "        '''\n",
    "        assert isinstance(policy, np.ndarray) and len(policy) == self.numStates\n",
    "        assert isinstance(num_trials, int) and num_trials>0\n",
    "        assert isinstance(timeLimit, bool)\n",
    "        assert isinstance(maxT, int) and maxT>0\n",
    "\n",
    "        \n",
    "        success_count = 0\n",
    "        for i in range(num_trials):\n",
    "            t = 0\n",
    "            state = int(self.env.reset()) #resetting state to initial position\n",
    "            while(True):\n",
    "                a = policy[state] #getting action from policy\n",
    "                next_state, r, done, info = self.env.step(a) #taking a step using action\n",
    "#                 print('next_state, r = ', next_state, r)\n",
    "                # Check if we reached goal, i.e. check for success\n",
    "                if (done and r == 1.0):\n",
    "                    success_count += 1\n",
    "                    break\n",
    "                \n",
    "                # Checking if we fell in a hole\n",
    "                if done:\n",
    "                    break\n",
    "                \n",
    "                if timeLimit and t>maxT:\n",
    "                    print('Max time exceeded. Breaking out of loop')\n",
    "                    break\n",
    "                \n",
    "                state = next_state\n",
    "                t += self.dt\n",
    "        return success_count/num_trials\n",
    "\n",
    "    def LearnModel(self, num_samples = 100000):\n",
    "        '''\n",
    "        Returns transition probabilities and reward function\n",
    "        \n",
    "        p(s'|a, s) is accessed by typing p[s][a][s']\n",
    "        r(s,a,s') is accessed by typing r[s][a][s']\n",
    "        '''\n",
    "        assert isinstance(num_samples, int) and num_samples > 0\n",
    "        \n",
    "        self.env.reset()\n",
    "        p = np.zeros((self.numStates, self.numActions, self.numStates))\n",
    "        r = np.zeros((self.numStates, self.numActions, self.numStates))\n",
    "        counter = np.zeros((self.numStates, self.numActions))\n",
    "        self.count_s = np.zeros(self.numStates)\n",
    "        self.count_a = np.zeros(self.numActions)\n",
    "        \n",
    "        for i in range(num_samples):\n",
    "            s = np.random.randint(low = 0, high = self.numStates, dtype = int)\n",
    "            a = np.random.randint(low = 0, high = self.numActions, dtype = int)\n",
    "            self.count_s[s] += 1; self.count_a[a] += 1\n",
    "            \n",
    "            self.env.unwrapped.s = s #setting current state to randomly chosen state\n",
    "            s_prime, reward, _, _ = self.env.step(a)\n",
    "            \n",
    "            p[s][a][s_prime] += 1\n",
    "            r[s][a][s_prime] += reward\n",
    "            counter[s][a] += 1\n",
    "        \n",
    "        #use itertools instead\n",
    "        for s in range(self.numStates):\n",
    "            for a in range(self.numActions):\n",
    "                assert counter[s][a] != 0, 'Zero occurences of state-action pair. Cannot divide by 0'\n",
    "                p[s][a][:] = p[s][a][:]/counter[s][a]\n",
    "                r[s][a][:] = r[s][a][:]/counter[s][a]\n",
    "        \n",
    "        # Checking that probabilities sum to 1        \n",
    "        for s in range(self.numStates):\n",
    "            for a in range(self.numActions):\n",
    "                assert abs(sum(p[s,a,:])-1.0) < 1e-4, 'Probabilities dont sum to 1 --> %f' % sum(p[s,a,:])\n",
    "        \n",
    "        return p, r, counter\n",
    "    \n",
    "    def initializeValueAndPolicyFunction(self, initial_value = 0):\n",
    "        '''\n",
    "        Initializes Value and Policy function and returns them\n",
    "        All initial values for Value function are set to param initial_value\n",
    "        Initial values for Policy function are chosen randomly between [0, number of actions)\n",
    "        '''\n",
    "        assert isinstance(initial_value, (int, float))\n",
    "        \n",
    "        V = np.zeros((self.numStates))\n",
    "        policy = np.zeros((self.numStates))\n",
    "        \n",
    "        for s in range(self.numStates):\n",
    "            a = np.random.randint(low = 0, high = self.numActions, dtype = int)\n",
    "            V[s] = initial_value\n",
    "            policy[s] = a\n",
    "        \n",
    "        return V, policy\n",
    "    \n",
    "    def evaluatePolicy(self, V, policy, gamma = 0.9):\n",
    "        '''\n",
    "        Evaluates policy for 1 iteration\n",
    "        '''\n",
    "        assert isinstance(V, np.ndarray) and len(V) == self.numStates\n",
    "        assert isinstance(gamma, float) and 0.0<gamma<1.0\n",
    "        \n",
    "        V_new = np.zeros_like(V)\n",
    "        for s in range(self.numStates):\n",
    "            a = policy[s]\n",
    "            pf_s = self.env.P[s][a] #prob distribution over states for taking action a at state s\n",
    "            # Calculating expected value of Value function coz of policy\n",
    "            exp_V = 0.0\n",
    "            for possible_sa_pairs in pf_s:\n",
    "                prob, next_state, r, _ = possible_sa_pairs\n",
    "                exp_V += prob*( r + gamma*V[next_state] ) \n",
    "            V_new[s] = exp_V\n",
    "        return V_new\n",
    "                \n",
    "    def checkPolicyConvergence(self, V_old, V_new, threshold = 1e-3):\n",
    "        '''\n",
    "        Checks for convergence by checking if the maximum difference between value of all states \n",
    "        is less than some threshold\n",
    "        '''\n",
    "        assert isinstance(threshold, float) and 0 < threshold < 1\n",
    "        assert isinstance(V_old, np.ndarray) and len(V_old) == self.numStates\n",
    "        assert isinstance(V_new, np.ndarray) and len(V_new) == self.numStates\n",
    "        \n",
    "        max_diff = np.max(np.abs(V_old - V_new))\n",
    "        success = max_diff<threshold\n",
    "        return success, max_diff\n",
    "    \n",
    "    def doValueIteration(self, threshold = 1e-3, gamma = 0.9, verbose = False):\n",
    "        ''' Implement value iteration '''\n",
    "        # 1. Initialize V\n",
    "        V, _ = self.initializeValueAndPolicyFunction()\n",
    "        # 2. Initialize delta\n",
    "        delta = np.inf\n",
    "        # 3. Improve V over 1 iteration and then repeat until delta is less than some threshold\n",
    "        i = 0\n",
    "        while(delta > threshold):\n",
    "            delta = 0\n",
    "            for s in range(self.numStates):\n",
    "                v = deepcopy(V[s])\n",
    "                allActionsResult = self.getExpectationOverAction(V, s, gamma)\n",
    "                assert allActionsResult.shape == (self.numActions, ), 'Needs to be a 1D ndarray'\n",
    "                V[s] = np.max(allActionsResult)\n",
    "                delta = max(delta, np.abs(v - V[s]))\n",
    "            \n",
    "            i += 1\n",
    "            # Getting policy from value function\n",
    "            policy = self.getPolicyFromValueFunction(V, gamma)\n",
    "            avg_success_rate = self.TestPolicy(policy, num_trials = 100, maxT = 5000)\n",
    "            print('avg_success_rate = ', avg_success_rate)\n",
    "        \n",
    "        if verbose:\n",
    "            print('Value iteration converged in {0} iterations with final delta = {1}'.format(i, delta))\n",
    "        \n",
    "        return V\n",
    "        \n",
    "    def getExpectationOverAction(self, V, s, gamma):\n",
    "        '''\n",
    "        Calculate the expected value of all actions at state s\n",
    "        '''\n",
    "        assert isinstance(s, int) and s in range(self.numStates)\n",
    "        assert isinstance(gamma, (int, float)) and 0<gamma<1\n",
    "        \n",
    "        allActionsResult = np.zeros((self.numActions))\n",
    "        for a in range(self.numActions):\n",
    "            result_for_sa = self.env.P[s][a]\n",
    "            for sa in result_for_sa:\n",
    "                prob, next_state, r, _ = sa\n",
    "                allActionsResult[a] += prob*( r + gamma*V[next_state] )\n",
    "        return allActionsResult\n",
    "    \n",
    "    # POSSIBLE BUG -----> different gamma values can be used while calculating value function and deriving policy\n",
    "    \n",
    "    def getPolicyFromValueFunction(self, V, gamma):\n",
    "        '''\n",
    "        Deriving optimal policy using value function\n",
    "        '''\n",
    "        assert isinstance(gamma, (int, float)) and 0<gamma<1\n",
    "        \n",
    "        # 1. Initialize policy\n",
    "        _, policy = self.initializeValueAndPolicyFunction()\n",
    "        # 2. Get optimal action for each state\n",
    "        for s in range(self.numActions):\n",
    "            allActionsResult = self.getExpectationOverAction(V, s, gamma)\n",
    "            assert allActionsResult.shape == (self.numActions, ), 'Needs to be a 1D ndarray'\n",
    "            policy[s] = np.argmax(allActionsResult)\n",
    "        return policy\n",
    "    \n",
    "#     def plotPolicy(self, policy):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment map:\n",
      "[[b'S' b'F' b'F' b'F']\n",
      " [b'F' b'H' b'F' b'H']\n",
      " [b'F' b'F' b'F' b'H']\n",
      " [b'H' b'F' b'F' b'G']]\n",
      "\n",
      "Success Rate for policy is: 0.020\n"
     ]
    }
   ],
   "source": [
    "fl = FrozenLake()\n",
    "# Forming test policy\n",
    "policyTest = np.zeros((fl.numStates))\n",
    "for s in range(fl.numStates):\n",
    "    policyTest[s] = (s+1)%4\n",
    "\n",
    "success_rate = fl.TestPolicy(policyTest, num_trials = 100, maxT = 5000)\n",
    "print('\\nSuccess Rate for policy is: %.3f' % (success_rate))\n",
    "# p, r, counter = fl.LearnModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_success_rate =  0.0\n",
      "avg_success_rate =  0.19\n",
      "avg_success_rate =  0.0\n",
      "avg_success_rate =  0.0\n",
      "avg_success_rate =  0.0\n",
      "avg_success_rate =  0.0\n",
      "avg_success_rate =  0.0\n",
      "avg_success_rate =  0.0\n",
      "avg_success_rate =  0.04\n",
      "avg_success_rate =  0.01\n",
      "avg_success_rate =  0.01\n",
      "avg_success_rate =  0.0\n",
      "avg_success_rate =  0.0\n",
      "avg_success_rate =  0.04\n",
      "avg_success_rate =  0.0\n",
      "avg_success_rate =  0.0\n",
      "avg_success_rate =  0.0\n",
      "avg_success_rate =  0.0\n",
      "avg_success_rate =  0.0\n",
      "avg_success_rate =  0.0\n",
      "avg_success_rate =  0.03\n",
      "avg_success_rate =  0.0\n",
      "avg_success_rate =  0.0\n",
      "Value iteration converged in 23 iterations with final delta = 0.0009297665029441571\n",
      "[[0.06428821 0.05807365 0.07231299 0.05356057]\n",
      " [0.08830336 0.         0.11127288 0.        ]\n",
      " [0.14298808 0.24613328 0.29877497 0.        ]\n",
      " [0.         0.37905097 0.63860174 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "gamma = 0.9\n",
    "V = fl.doValueIteration(gamma = gamma, verbose=True)\n",
    "\n",
    "print(V.reshape(4,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 3. 0. 3.]\n",
      " [0. 0. 3. 1.]\n",
      " [1. 0. 3. 3.]\n",
      " [2. 1. 1. 2.]]\n"
     ]
    }
   ],
   "source": [
    "policy = fl.getPolicyFromValueFunction(V, gamma = gamma)\n",
    "print(policy.reshape(4,4))\n",
    "\n",
    "# LEFT = 0\n",
    "# DOWN = 1\n",
    "# RIGHT = 2\n",
    "# UP = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fl.TestPolicy(policy, num_trials = 100, maxT = 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plotting transition probabilites for a certain action a\n",
    "# fig, axs = plt.subplots(figsize = (6,6))\n",
    "# axs = plt.imshow(p[:,0,:], cmap='gray')\n",
    "# plt.colorbar(axs,fraction=0.046, pad=0.04)\n",
    "## Checking if states and actions were sampled uniformly\n",
    "# fig, axs = plt.subplots(1,2, figsize = (15,5))\n",
    "# axs[0].bar(range(fl.numStates), fl.count_s)\n",
    "# axs[1].bar(range(fl.numActions), fl.count_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir(gym.envs.toy_text.discrete.DiscreteEnv)\n",
    "# import inspect\n",
    "# inspect.getmembers(gym.envs.toy_text.discrete.DiscreteEnv, lambda a: not(inspect.isroutine(a)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ece276c_venv",
   "language": "python",
   "name": "ece276c_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
